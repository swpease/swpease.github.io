<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Scott's Stuff</title><link href="https://swpease.github.io/" rel="alternate"></link><link href="https://swpease.github.io/feeds/all.atom.xml" rel="self"></link><id>https://swpease.github.io/</id><updated>2025-06-23T00:00:00-07:00</updated><entry><title>PA Well Water, Part 6: Analyzing the Data</title><link href="https://swpease.github.io/pa-well-water-part-6-analyzing-the-data.html" rel="alternate"></link><published>2025-06-23T00:00:00-07:00</published><updated>2025-06-23T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2025-06-23:/pa-well-water-part-6-analyzing-the-data.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post continues from &lt;a href="https://swpease.github.io/pa-well-water-part-5-estimating-the-outcome-variable.html"&gt;Part 5&lt;/a&gt;. At 
long last, we've got our pieces in place. Time to plug it into our 
processing method!&lt;/p&gt;
&lt;h1&gt;Method&lt;/h1&gt;
&lt;p&gt;The actual code for what follows is &lt;a href="https://github.com/swpease/well_water/blob/master/R/BPR_CAR.Rmd"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I used the following clustering covariates: "EP_POV150", "EP_UNEMP", 
"EP_NOHSDP", "EP_AGE65", "EP_AGE17", "EP_MINRTY", "EP_HBURD". These are the 
Estimated …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post continues from &lt;a href="https://swpease.github.io/pa-well-water-part-5-estimating-the-outcome-variable.html"&gt;Part 5&lt;/a&gt;. At 
long last, we've got our pieces in place. Time to plug it into our 
processing method!&lt;/p&gt;
&lt;h1&gt;Method&lt;/h1&gt;
&lt;p&gt;The actual code for what follows is &lt;a href="https://github.com/swpease/well_water/blob/master/R/BPR_CAR.Rmd"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I used the following clustering covariates: "EP_POV150", "EP_UNEMP", 
"EP_NOHSDP", "EP_AGE65", "EP_AGE17", "EP_MINRTY", "EP_HBURD". These are the 
Estimated Percentage of the population that is: below 150% poverty, 
unemployed, without a high school diploma, over 65, under 17, minority, and housing cost-burdened
occupied housing units with annual income less than $75,000 (30%+ of income 
spent on housing costs). The housing burden is new, and replaces the median 
income component that was used in Wamsley, and poverty has changed from 100% 
to 150% federal poverty level. See the SVI 2020 documentation for 
details (SVI 2020 Updates section).&lt;/p&gt;
&lt;p&gt;The outcome variable (estimated # of well water users) and confounder (population density) are 
described in the bulleted list, below.&lt;/p&gt;
&lt;p&gt;I diverged from Wamsley in a few places. First, some SVI indicators changed 
(see above). I also excluded the three non-SVI ACS covariates, since they were 
non-SVI. I also considered the population density to be an alternative to 
county, so excluded county.&lt;/p&gt;
&lt;p&gt;I used the Premium package (officially spelled obnoxiously) for the Bayesian 
profile regression with conditional autocorrelation. I performed the 
following preprocessing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;removed census tracts with estimated populations of zero (both via ACS 
    and via CEISIN)&lt;/li&gt;
&lt;li&gt;generated an INLA file for the CAR component&lt;/li&gt;
&lt;li&gt;created the outcome, estimated # of well water users, by multiplying the 
    ACS population estimate (&lt;code&gt;E_TOTPOP&lt;/code&gt;) by the fraction of the 
    population-as-calculated-by-CEISIN-maps (see prior blog post) that 
    relies on well water&lt;/li&gt;
&lt;li&gt;created the population density, as tract population divided by land area 
    (&lt;code&gt;E_TOTPOP&lt;/code&gt; / &lt;code&gt;ALAND&lt;/code&gt;), to use as a confounder for the outcome component 
    of the model&lt;/li&gt;
&lt;li&gt;replaced the &lt;code&gt;-999&lt;/code&gt; encodings for missing clustering covariates data with 
    &lt;code&gt;NA&lt;/code&gt;s&lt;/li&gt;
&lt;li&gt;converted the clustering covariates into quartiles&lt;ul&gt;
&lt;li&gt;Premium can handle discrete or Gaussian cases, and these are not Gaussian.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the final model's outcome component (i.e. excluding the clustering bit) 
looks like:&lt;/p&gt;
&lt;div class="math"&gt;$$ 
  y_i \sim Poisson(\mu_i) \\
  log(\mu_i) = \theta_{Z_i} + \beta\rho_i + log(n_i) + u_i
$$&lt;/div&gt;
&lt;p&gt;Well, I actually used a negative binomial model, but whatever. Anyway, so 
&lt;span class="math"&gt;\(\mu_i\)&lt;/span&gt; is the expected number of well water users. 
&lt;span class="math"&gt;\(\theta_{Z_i}\)&lt;/span&gt; is a categorical predictor (a random/varying intercept I 
think), with &lt;span class="math"&gt;\(Z_i\)&lt;/span&gt; indicating the group to which &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; belongs. 
&lt;span class="math"&gt;\(\beta\rho_i\)&lt;/span&gt; is the confounder, population density acting to capture rurality. 
&lt;span class="math"&gt;\(log
(n_i)\)&lt;/span&gt; 
is the offset (&lt;span class="math"&gt;\(n_i\)&lt;/span&gt; is the total census tract population). &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; is 
the 
spatially structured error term. &lt;/p&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;Tada!... :(&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 1" src="https://swpease.github.io/images/science/well_water/clusters.png"&gt;&lt;/p&gt;
&lt;p&gt;Wait, what? Did I do something wrong? Well, I can't for sure say "No.", but I 
subsequently tried a &lt;a href="https://github.com/swpease/well_water/tree/master/R"&gt;bunch of different modifications&lt;/a&gt; to the BPR to try and 
get some sort of clustering. No luck.&lt;/p&gt;
&lt;h1&gt;What Happened?&lt;/h1&gt;
&lt;p&gt;So I finally did what I'd ordinarily do right off the bat: plot my data. 
I'll look at the clustering covariates each against the fraction of the 
census tract that is thought to rely on well water. That should do the trick!
Let's see:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 2" src="https://swpease.github.io/images/science/well_water/eppov150.png"&gt;&lt;/p&gt;
&lt;p&gt;Oh dear. Well, maybe the other covariates look better?&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 3" src="https://swpease.github.io/images/science/well_water/other_clustering_covs.png"&gt;&lt;/p&gt;
&lt;p&gt;Hm. Not exactly the "complex and non-linear" relationships I was led to 
believe existed. Well maybe it's something 
that takes multiple 
dimensions to see. How about a PCA biplot &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;?&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 4" src="https://swpease.github.io/images/science/well_water/biplot.png"&gt;&lt;/p&gt;
&lt;p&gt;It's quite blobby. I suppose it at least indicates that there's some 
positive relationship between being old and using well water. That ties out 
with rural populations being a bit older.&lt;/p&gt;
&lt;p&gt;All I can really see in the plots against the proportion of well water users 
("frac_WW_CEISIN") is that the boundaries are driving any correlations. The 
fully urban (fully on a PWS) areas span the range of values for every 
predictor, and the 
fully rural (fully on well water) areas also have a somewhat wider range of 
values compared to the intermediate values.&lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;Well that was a let-down. I'm not sure where I might have gone wrong. If I 
didn't make any mistakes, then how the heck did they pull off their results? 
I keep double checking my variables. Like in this post, I keep thinking, "the 
fraction of well water users is the right y-axis variable, right?".&lt;/p&gt;
&lt;p&gt;I suppose it's of interest that well water users in PA run the gamut with 
respect to social vulnerability indicators.&lt;/p&gt;
&lt;p&gt;Um... I guess at least I learned some things.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;I haven't use one of these in a while, so had to refresh on the 
details. I found two good sources were a &lt;a href="https://blogs.sas.com/content/iml/2019/11/06/what-are-biplots.html"&gt;SAS blog post&lt;/a&gt; and the &lt;a href="https://friendly.github.io/ggbiplot/reference/ggbiplot.html"&gt;{ggbiplot} docs&lt;/a&gt;. The displayed 
plot is actually from {ggfortify}, using its autoplot, 
but all the plots I toyed around with look virtually the same.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Science"></category><category term="analysis"></category></entry><entry><title>PA Well Water, Part 5: Estimating the Outcome Variable</title><link href="https://swpease.github.io/pa-well-water-part-5-estimating-the-outcome-variable.html" rel="alternate"></link><published>2025-06-16T00:00:00-07:00</published><updated>2025-06-16T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2025-06-16:/pa-well-water-part-5-estimating-the-outcome-variable.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post expands on &lt;a href="https://swpease.github.io/pa-well-water-part-3.html"&gt;Part 3&lt;/a&gt; -- specifically the 
section concerning Outcome Approximations -- and begins preparing data for 
analysis.&lt;/p&gt;
&lt;p&gt;In this post, I will compare the estimation method used in the paper vs an 
alternative, and then look at a few other published methods. The estimation is 
for &lt;strong&gt;the 
outcome …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post expands on &lt;a href="https://swpease.github.io/pa-well-water-part-3.html"&gt;Part 3&lt;/a&gt; -- specifically the 
section concerning Outcome Approximations -- and begins preparing data for 
analysis.&lt;/p&gt;
&lt;p&gt;In this post, I will compare the estimation method used in the paper vs an 
alternative, and then look at a few other published methods. The estimation is 
for &lt;strong&gt;the 
outcome variable, which 
is: the number 
of people who rely on domestic well water, i.e. who do not have coverage by 
a public water system (PWS) at their residence&lt;/strong&gt;. In the paper, they use 
number of 
households for some reason. I don't see what the practical difference to 
individuals vs households might be, so I'll just leave it as individuals.&lt;/p&gt;
&lt;h2&gt;The Two Estimation Methods&lt;/h2&gt;
&lt;p&gt;Comparing their method (1) vs an alternate (2):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Area-based estimation: The proportion of the census tract's area that is not 
   covered by a PWS 
   equals the proportion of the census tract's population that relies on domestic 
   well water. The census tract's population is provided by the SVI data 
   (which is taken from the ACS).&lt;/li&gt;
&lt;li&gt;Population density-based estimation: Use high-resolution population density 
   estimates to 
   directly count the 
   estimated number of people who rely on domestic well water, again based 
   on PWS coverage. This value 
   can be used in two ways:&lt;ol&gt;
&lt;li&gt;Directly.&lt;/li&gt;
&lt;li&gt;Use the same population density estimate data to determine the 
   estimated population for the whole census tract, take the proportion of 
   that total that is on domestic well water, and apply said proportion to 
   the ACS population estimate. This might be better, as it would -- 
   assuming the population ratio of PWS/well-water remains consistent -- 
   let us apply this proportion to what should be a better 
   population estimate: the ACS estimate. ACS data is more recent (2022 
   vs 2019).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Methods (in the Paper-y Sense)&lt;/h1&gt;
&lt;p&gt;Getting these estimates of population relying on well water required working 
with several map datasets (Shapefiles, GeoTIFF...). I initially conducted 
much of the data processing in QGIS. I want to translate it to scripting (in 
Python) for reproducibility/defensibility/recall, but only have a part of it 
in Python so far.&lt;/p&gt;
&lt;h2&gt;Data Sources&lt;/h2&gt;
&lt;p&gt;Firstly, the four data sources I collected:&lt;/p&gt;
&lt;h3&gt;SVI Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;src: https://www.atsdr.cdc.gov/place-health/php/svi/svi-data-documentation-download.html&lt;/li&gt;
&lt;li&gt;params:&lt;ul&gt;
&lt;li&gt;Year = 2022&lt;/li&gt;
&lt;li&gt;Geography = Pennsylvania&lt;/li&gt;
&lt;li&gt;Geography Type = Census Tracts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;PA Public Water Systems (PA PWS)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;src: https://newdata-padep-1.opendata.arcgis.com/datasets/PADEP-1::public-water-systems-public-water-supplier-service-areas/about&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;TIGER/Line Shapefile (T/L)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;src: https://www.census.gov/cgi-bin/geo/shapefiles/index.php&lt;/li&gt;
&lt;li&gt;params:&lt;ul&gt;
&lt;li&gt;Year = 2022&lt;/li&gt;
&lt;li&gt;Layer Type = Census Tracts&lt;/li&gt;
&lt;li&gt;State = Pennsylvania&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Population Density Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;src: https://data.humdata.org/dataset/united-states-high-resolution-population-density-maps-demographic-estimates#&lt;/li&gt;
&lt;li&gt;refs: &lt;ul&gt;
&lt;li&gt;https://dataforgood.facebook.com/dfg/tools/high-resolution-population-density-maps&lt;/li&gt;
&lt;li&gt;https://ciesin.climate.columbia.edu/programs#!#cu_card_group_media-400&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;files:&lt;ul&gt;
&lt;li&gt;population_usa38_-80.tif.zipGeoTIFF&lt;/li&gt;
&lt;li&gt;population_usa38_-90.tif.zipGeoTIFF&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;citation:&lt;ul&gt;
&lt;li&gt;Facebook Connectivity Lab and Center for International Earth Science 
 Information 
Network - CIESIN - Columbia University. 2016. High Resolution Settlement 
Layer (HRSL). Source imagery for HRSL © 2016 DigitalGlobe. Accessed 01 
06 2025. &lt;/li&gt;
&lt;li&gt;Please note that as of 2024, Meta’s high resolution population 
density maps are no longer being updated.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Processing Outline&lt;/h2&gt;
&lt;p&gt;I did some of the processing in QGIS, and some in Python. I'll link to the 
Python file, and outline the QGIS steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Join SVI and T/L by census tract ID&lt;/li&gt;
&lt;li&gt;Fix geometries on PA PWS&lt;/li&gt;
&lt;li&gt;Reproject PA PWS to match T/L-SVI join&lt;/li&gt;
&lt;li&gt;Difference T/L-SVI using PA PWS overlay&lt;ul&gt;
&lt;li&gt;Yields well water region (WWR) layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create virtual raster from Pop Dens GeoTIFFs&lt;/li&gt;
&lt;li&gt;Reproject T/L-SVI and WWR to match Pop Dens&lt;/li&gt;
&lt;li&gt;Zonal stats (sum) using Pop Dens virtual raster and&lt;ul&gt;
&lt;li&gt;T/L-SVI  =&amp;gt; pop est of the census tract&lt;/li&gt;
&lt;li&gt;WWR  =&amp;gt; pop est of the well water reliant parts of the census tract&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From there, I used Python (see the &lt;a href="https://github. com/swpease/well_water/tree/master/python"&gt;repository&lt;/a&gt;).&lt;/p&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;Note that I am comparing method 1 to method 2.2 (above).&lt;/p&gt;
&lt;h2&gt;Total Well Water Population&lt;/h2&gt;
&lt;p&gt;Firstly, what do these methods say the total PA well water population is?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Area based: 3.92 million&lt;/li&gt;
&lt;li&gt;Pop density based: 2.42 million&lt;ul&gt;
&lt;li&gt;Note: the two alternative methods I outlined above were within 50k of 
  each other (~2%), as method 2.1 (direct usage of pop #) was 2.38M.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are way different! Are either of these more likely to be accurate? Is 
there a third party? (Yes). It didn't occur to me to see if someone had 
already estimated these values until just now, so I'll have to see what they 
say after I finish this next part.&lt;/p&gt;
&lt;p&gt;Continuing on this specific comparison...&lt;/p&gt;
&lt;h2&gt;Comparative Plots&lt;/h2&gt;
&lt;p&gt;We've already established that these estimates are way different. Is there 
any patten to the difference?&lt;/p&gt;
&lt;p&gt;First, let's see the numbers at the census tract level as proportions of the 
total population:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 1" src="https://swpease.github.io/images/science/well_water/ww_pop_comp_prop.png"&gt;&lt;/p&gt;
&lt;p&gt;And second, as total population estimates:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 2" src="https://swpease.github.io/images/science/well_water/ww_pop_comp_popul.png"&gt;&lt;/p&gt;
&lt;p&gt;We already knew the area-based values were going to generally exceed the 
population-density-based values. I was just kind of hoping there might be a 
pattern to it.&lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;h2&gt;Total Well Water Population (Now with 3rd Parties)&lt;/h2&gt;
&lt;p&gt;The PA Dept of 
Conservation and Natural Resources states that &lt;a href="https://www.pa.gov/agencies/dcnr/conservation/water/groundwater.html"&gt;over 1 million&lt;/a&gt; homeowners depend on 
private wells for drinking water. I suppose then the question is: how big is 
a home? The US Census Bureau's estimate for PA in 2019-2023 is &lt;a href="https://www.census.gov/quickfacts/fact/table/PA/INC110223"&gt;2.40 persons 
per household&lt;/a&gt;. 
So a rough estimate is &lt;strong&gt;2.4 million&lt;/strong&gt; well users, though there are factors 
like 
rural areas having larger average household sizes.  &lt;/p&gt;
&lt;p&gt;Penn State estimates &lt;a href="https://extension.psu.edu/private-water-systems-faqs"&gt;3.5 million&lt;/a&gt;, though I don't see 
their source. Penn State also estimates &lt;a href="https://extension.psu.edu/social-science-of-drinking-water"&gt;1-3.5 million&lt;/a&gt;, this time with some good sources 
that I'll look into and discuss later, though I'm wondering if they 
subconsciously misread "homes" as "people" for the 1M case.&lt;/p&gt;
&lt;p&gt;It turns out that "later" is now.&lt;/p&gt;
&lt;h3&gt;Johnson Papers&lt;/h3&gt;
&lt;p&gt;There were a couple of related papers &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; that estimated the well water 
reliant population across the entire contiguous US, at a 1km^2 resolution, 
for 1990, 2000, and 2010. Apparently 1990 is the last time the US Census 
asked people about their water sources, so they extrapolated from there. 
I've read through the first only, but it presents some interesting 
information:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;it appears that the number of people supplied by domestic-wells has plateaued and may in fact be dropping.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The proportion of the population dependent upon domestic wells (domestic ratio) is slowly decreasing nationally.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Their own estimates for Pennsylvania's well water reliant populations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1990: 2.43 million &lt;sup id="fnref2:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; (see Table 3)&lt;/li&gt;
&lt;li&gt;2000: 2.29 million (calculated via Zonal statistics (sum) of the PA 
    TIGER/Line 
    shapefile, dissolved to all of PA, on their REM 
    map &lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;)&lt;/li&gt;
&lt;li&gt;2010: &lt;strong&gt;2.20 million&lt;/strong&gt; (same calculation)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Murray&lt;/h3&gt;
&lt;p&gt;Murray &lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt; provides yet another national estimate of well water reliance, 
but looking at housing units. I haven't even skimmed this one yet, but it 
estimates that in 2010, &lt;strong&gt;1.25 million housing units&lt;/strong&gt; had private wells in PA 
(Figure 9). This doesn't disagree with the estimate by the PA Dept of 
Conservation and Natural Resources.&lt;/p&gt;
&lt;p&gt;In trying to compare the population-density-based estimates to this, which was 
easy 
enough because ACS (in the SVI) has housing unit estimates as well, I wound 
up with an estimated 1.75 million housing units. That number seems high, 
especially considering my population estimate of 2.4 million, but it's what 
the ACS says, so... &lt;/p&gt;
&lt;h2&gt;So Which Estimate to Use?&lt;/h2&gt;
&lt;p&gt;Given the results by Murray and Johnson, my inclination is that the 
population-density-based estimates of well water 
users might be an overestimate, while the area-based estimate is 
likely a huge overestimate.&lt;/p&gt;
&lt;p&gt;While the population-density-based estimates can be converted to either 
population or housing units, I am leaning towards population because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(I know 
this is dumb) it's what I first thought of.&lt;/li&gt;
&lt;li&gt;I am interested in people, not housing.&lt;/li&gt;
&lt;li&gt;I'm concerned converting to housing units might lead to some extra 
    uncertainty, whereas the CEISIN-direct population estimates (method 2.2) 
    closely 
    align with the ACS-based ones (method 2.1) (see supplemental graphs &lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt;).&lt;/li&gt;
&lt;li&gt;It seems to more closely align with a third party (Johnson).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How is the alignment exactly? Let's do another scatter plot:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 3" src="https://swpease.github.io/images/science/well_water/ww_pop_comp_Johnson.png"&gt;&lt;/p&gt;
&lt;p&gt;A spot check of some of the largest discrepancies seems to indicate that the 
population-density-based estimates are better: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cases with large Johnson 
estimates and small population-density-based estimates seem to be tracts 
that are covered by a PWS. &lt;/li&gt;
&lt;li&gt;cases with small Johnson 
estimates and large population-density-based estimates have little to no PWS 
    coverage, so I think it's just a question of population estimates, which 
    I'll go with the recent ACS data for.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Factoring all of this together, I want to &lt;strong&gt;first just use the 
population-density-based estimates&lt;/strong&gt;. I will want to also try the Johnson 
numbers, to see how that compares.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I have some estimates for the outcome variable now. That's good! I think I'm 
ready to fire up Premium and see what this baby can do.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Johnson, Tyler &amp;amp; Belitz, Kenneth &amp;amp; Lombard, Melissa. (2019). 
Estimating domestic well locations and populations served in the contiguous 
U.S. for years 2000 and 2010. Science of The Total Environment. 687. 10.
1016/j.scitotenv.2019.06.036. &lt;a href="https://www.researchgate.net/publication/333728321_Estimating_domestic_well_locations_and_populations_served_in_the_contiguous_US_for_years_2000_and_2010"&gt;open source&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Tyler D. Johnson, Kenneth Belitz, Domestic well locations and 
populations served in the contiguous U.S.: 1990, Science of The Total 
Environment, Volumes 607–608, 2017, Pages 658-668, ISSN 0048-9697. https://doi.org/10.1016/j.scitotenv.2017.07.018. &lt;a href="https://www.researchgate.net/publication/318446309_Domestic_well_locations_and_populations_served_in_the_contiguous_US_1990"&gt;open source&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Johnson, T.D., and Belitz, K., 2019, Domestic well locations and populations served in the contiguous U.S.: datasets for decadal years 2000 and 2010: U.S. Geological Survey data release, https://doi.org/10.5066/P9FSLU3B.&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;Murray, A., A. Hall, J. Weaver, and F. Kremer. 2021. "Methods for Estimating Locations of Housing Units Served by Private Domestic Wells in the United States Applied to 2010." Journal of the American Water Resources Association 57 (5): 828–843. https://doi.org/10.1111/1752-1688.12937.&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;&lt;img alt="Figure 4" src="https://swpease.github.io/images/science/well_water/ww_pop_comp_CEISIN_methods.png"&gt;
&lt;img alt="Figure 5" src="https://swpease.github.io/images/science/well_water/ww_CEISIN_HU_vs_pop.png"&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Science"></category><category term="analysis"></category></entry><entry><title>PA Well Water, Part 4</title><link href="https://swpease.github.io/pa-well-water-part-4.html" rel="alternate"></link><published>2025-06-10T00:00:00-07:00</published><updated>2025-06-10T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2025-06-10:/pa-well-water-part-4.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post continues from &lt;a href="https://swpease.github.io/pa-well-water-part-3.html"&gt;Part 3&lt;/a&gt; and covers the
Bayesian profile regression model.&lt;/p&gt;
&lt;h1&gt;Bayesian Profile Regression&lt;/h1&gt;
&lt;h2&gt;What Is It?&lt;/h2&gt;
&lt;p&gt;There were several resources I used to get a grasp on the method. Molitor et 
al. &lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; present kind of the founding proposal of the method, and this is a …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post continues from &lt;a href="https://swpease.github.io/pa-well-water-part-3.html"&gt;Part 3&lt;/a&gt; and covers the
Bayesian profile regression model.&lt;/p&gt;
&lt;h1&gt;Bayesian Profile Regression&lt;/h1&gt;
&lt;h2&gt;What Is It?&lt;/h2&gt;
&lt;p&gt;There were several resources I used to get a grasp on the method. Molitor et 
al. &lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; present kind of the founding proposal of the method, and this is a 
surprisingly readable paper. &lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; and &lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt; were helpful in my understanding 
the spatial component that can be added to these models. &lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt; was helpful as 
yet another writeup of the method, and especially for including a DAG of the 
full model.&lt;/p&gt;
&lt;p&gt;There is an R package for this method, called PReMiuM, which I will
henceforth call Premium, because fuck that. &lt;/p&gt;
&lt;p&gt;It took me a little while to understand how this method works. The gist is
that you fit a joint model of a clustering component and a regression
component. For each observation, its cluster assignment acts as a
categorical predictor&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; for the regression on your outcome variable. The
joint nature of the model means that the outcome variable influences 
clustering. I know that's confusing to read. Just read through those sources I 
listed. &lt;/p&gt;
&lt;p&gt;Molitor lists four benefits of the method, which I will summarize/combine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The benefits of Bayesian methods re uncertainty and posterior analyses.&lt;/li&gt;
&lt;li&gt;The clustering method that it uses lets the model figure out how many 
    there should be, rather than requiring you to pre-specify&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;"the outcome and the clusters mutually inform each other".&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That last point is the crux of my issue with this SVI paper.&lt;/p&gt;
&lt;h2&gt;Why Use It?&lt;/h2&gt;
&lt;p&gt;Molitor proposes it as a complement/alternative to standard regression:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Standard regression analyses are often plagued with problems encountered when one tries to make meaningful inference going beyond main effects, using datasets that contain dozens of variables that are potentially correlated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I guess one might say that the world isn't made up of convenient little 
lines. This method works around this problem by:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;adopt a more global point of view, where inference is based on clusters representing covariate patterns as opposed to individual risk factors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Re regression vs profile regression, Molitor concludes with:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Since the two approaches address different characteristics of association, both should be used in a complementary fashion to progress our understanding of the association between an outcome and a set of correlated covariates.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another nice explanation is the Introduction in Belloni&lt;sup id="fnref2:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt;, which also 
(briefly) compares it to other methods that might do a similar thing, such 
as random forests or principal component regression. &lt;/p&gt;
&lt;h2&gt;Comments&lt;/h2&gt;
&lt;p&gt;It doesn't seem to be a super popular analysis method. I noticed a lot of 
repeat names on the papers I looked at. I suspect a lot of people are 
reaching for machine learning methods.&lt;/p&gt;
&lt;h1&gt;The Problem&lt;/h1&gt;
&lt;p&gt;Per Wamsley:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we did not include the response model when determining clusters because domestic well prevalence is clustered by county, and the BPR does not allow for such spatial clustering effects modeling. Instead, the clusters of exposure profiles were fit in a second-stage multilevel regression model to estimate the relationship between social vulnerability profiles with domestic well prevalence&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I take this to mean that there were two steps, the clustering and the 
regression, but importantly, they were separate things entirely, meaning the 
regression model had zero influence on the clustering. However, Bayesian 
profile regression is 
supposed to fit these components jointly. Per Molitor:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[it] fits the model as a unit, allowing an individual’s outcome to 
potentially influence cluster membership&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And to hear it from another source &lt;sup id="fnref3:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;one of the principal motivations for PRM models is that the disease outcome influences cluster membership so that they can inform each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And to hear it from yet another source &lt;sup id="fnref:7"&gt;&lt;a class="footnote-ref" href="#fn:7"&gt;7&lt;/a&gt;&lt;/sup&gt; (the middle two intro paragraphs 
might be good to read):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;it may be desirable to make use of (potentially highly informative) 
outcome information directly, in order to guide inference toward the most 
relevant clustering structures. That is, we may wish to use the outcome 
information during the clustering analysis itself, rather than during post-analysis validation. This
is one of the principal motivations for Bayesian profile regression&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This change can cause radically different clustering.&lt;/p&gt;
&lt;h2&gt;Toy Example&lt;/h2&gt;
&lt;p&gt;I'll fit two models: one as a standard Bayesian profile regression, the 
second just the covariate clustering aspect as might happen in a two-phase 
modeling approach as I think Wamsley used. &lt;/p&gt;
&lt;p&gt;Here is some simulated data of a 
Poisson outcome with a couple of correlated predictors:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;set.seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;700&lt;/span&gt;
&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;rnorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# get a second distinct cluster&lt;/span&gt;
&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;rnorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;ifelse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;rbinom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;ifelse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;rpois&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dummy_col&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# for &amp;quot;offset&amp;quot; (+ log(dummy_col) as pred ==&amp;gt; + log(1) == + 0 on rhs of glm)&lt;/span&gt;
&lt;span class="n"&gt;obs_dat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;data.frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;dummy_col&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can see how it looks with &lt;code&gt;pairs(obs_dat[,1:3])&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 1" src="https://swpease.github.io/images/science/well_water/pairs_plot.png"&gt;&lt;/p&gt;
&lt;h3&gt;Standard Bayesian Profile Regression Usage&lt;/h3&gt;
&lt;p&gt;Fitting it with Premium:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mod_includeY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PReMiuM&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;profRegr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;yModel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Poisson&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;xModel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Normal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;outcomeT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dummy_col&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;nClusInit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# excludeY = TRUE,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;obs_dat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;pois_includeY&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;covNames&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;x1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;x2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;outcome&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;12345&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The clustering looks like so:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 2" src="https://swpease.github.io/images/science/well_water/include_y_1.png"&gt;
&lt;img alt="Figure 3" src="https://swpease.github.io/images/science/well_water/include_y_2.png"&gt;&lt;/p&gt;
&lt;p&gt;This output shows the two main things about this method:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The separate purple cluster in the x1 vs x2 plot shows that if the 
     covariate profiles are distinct, they will be clustered separately even 
     if the outcomes are similar (see how purple and blue/green are mixed in 
     the plot of x1 vs y).&lt;/li&gt;
&lt;li&gt;The regression component (&lt;code&gt;yModel="Poisson"&lt;/code&gt;), &lt;em&gt;NOT&lt;/em&gt; the y data per 
     se!,
     splits up the oblong 
     cluster into roundish 
     blobs. The 
     red/green/blue section is obviously one big cluster as viewed on x1 vs 
     x2's plot, 
     but using it as a 
     predictor for y would yield terribly wide predictions. The y data as 
     part of the linear model
     provides evidence that this one big cluster might be better off split 
     up, as happens here. &lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Excluding the y Linear Model Component&lt;/h3&gt;
&lt;p&gt;Now we use the &lt;code&gt;excludeY&lt;/code&gt; argument to essentially just cluster via the 
covariates (&lt;a href="https://github.com/silvialiverani/PReMiuM-R-package/issues/13"&gt;I think&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;mod_excludeY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;PReMiuM&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;profRegr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;yModel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Poisson&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;xModel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Normal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;outcomeT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dummy_col&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;nClusInit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;excludeY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;obs_dat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;pois_excludeY&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;covNames&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;x1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;x2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;outcome&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;12345&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Which yields:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 4" src="https://swpease.github.io/images/science/well_water/exclude_y_1.png"&gt;
&lt;img alt="Figure 5" src="https://swpease.github.io/images/science/well_water/exclude_y_2.png"&gt;&lt;/p&gt;
&lt;p&gt;So now we see that the big oblong cluster stays in one piece. y's linear model 
isn't there 
to influence it to split up. Note that this clustering remains (in this case)
even if we add y into &lt;code&gt;covNames&lt;/code&gt; like &lt;code&gt;covNames = c("x1", "x2", "y")&lt;/code&gt;: the clustering just says "yep, that's a 
cluster", because it doesn't have the linear model to tell it that this 
clustering yields poor predictions. I didn't include that output because it 
looks the same as this.&lt;/p&gt;
&lt;h2&gt;So.. What's the Issue in This Case?&lt;/h2&gt;
&lt;p&gt;Cluster 15.&lt;/p&gt;
&lt;p&gt;Cluster 15 winds up having the highest expected users of well water per 
their model. However,
empirically it is comprised of census tracts with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A median of 0 houses with wells.&lt;/li&gt;
&lt;li&gt;A 3rd quartile of 12 houses with wells.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Bayesian profile regression, those census tracts in Cluster 15 with high 
well usage (which is at most a 
quarter of said tracts) ought to be a separate cluster, and would be under a 
joint model as Bayesian profile regression implements. They might have 
similar covariate profiles, but the clustering would be (probably quite) 
different.&lt;/p&gt;
&lt;p&gt;I think that the adjustment for county must have done some heavy lifting to 
get Cluster 15 into the top spot. Maybe something like the tracts with lots 
of well users were in counties with generally low well usage? &lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I think I've made a good argument for there being a mistake in the analysis. 
At least, for them not yielding results that Bayesian profile regression 
should yield (not that profile regression is "the truth").
Perhaps I've misread/misinterpreted their methods, but the wonkiness of 
Cluster 15 matches with what would go differently in a joint vs 2-phase model.&lt;/p&gt;
&lt;p&gt;Next I suppose I ought to try and do the analysis with a joint model. See 
how that goes.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Well, Molitor in &lt;sup id="fnref2:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; says "The previously described assignment model 
clusters individuals into groups and these cluster assignments can be 
simultaneously used as categorical predictors of an outcome.", however when 
I said this to &lt;em&gt;the&lt;/em&gt; Liverani of Premium fame, she said &lt;a href="https://github.com/silvialiverani/PReMiuM-R-package/issues/13#issuecomment-2838078844"&gt;that I was wrong&lt;/a&gt;. I'm sticking with Molitor on this one.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Molitor, J., Papathomas, M., Jerrett, M., Richardson, S., 2010. 
Bayesian profile
regression with an application to the National survey of children’s health.
Biostatistics 11, 484–498. https://doi.org/10.1093/biostatistics/kxq013.&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Silvia Liverani, Aurore Lavigne, Marta Blangiardo, Modelling collinear and spatially correlated data, Spatial and Spatio-temporal Epidemiology, Volume 18, 2016, Pages 63-73, ISSN 1877-5845, https://doi.org/10.1016/j.sste.2016.04.003. (https://www.sciencedirect.com/science/article/pii/S1877584515300411)&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;Coker ES, Molitor J, Liverani S, Martin J, Maranzano P, Pontarollo N, et al. Bayesian profile regression to study the ecologic associations of correlated environmental exposures with excess mortality risk during the first year of the Covid-19 epidemic in lombardy, Italy. Environ Res 216(Pt 1):114484. Available from: https://europepmc.org/articles/PMC9547389 https:// doi.org/10.1016/j.envres.2022.114484 PMID: 36220446&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;Belloni M, Laurent O, Guihenneuc C and Ancelet S (2020) Bayesian 
Profile Regression to Deal With Multiple Highly Correlated Exposures and a Censored Survival Outcome. First Application in Ionizing Radiation Epidemiology. Front. Public Health 8:557006. doi: 10.3389/fpubh.2020.557006&amp;#160;&lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;The Dirichlet process mixture model (DPMM), which you can learn about 
on &lt;a href="https://www.pymc.io/projects/examples/en/latest/mixture_models
/dp_mix.html"&gt;PyMC&lt;/a&gt; or see a detailed mathematical walkthrough for the 
non-statistician &lt;a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6583910/"&gt;here&lt;/a&gt;. 
Those resources helped me, at least.&amp;#160;&lt;a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:7"&gt;
&lt;p&gt;Anaïs Rouanet, Rob Johnson, Magdalena Strauss, Sylvia Richardson, Brian D Tom, Simon R White, Paul D W Kirk, Bayesian profile regression for clustering analysis involving a longitudinal response and explanatory variables, Journal of the Royal Statistical Society Series C: Applied Statistics, Volume 73, Issue 2, March 2024, Pages 314–339, https://doi.org/10.1093/jrsssc/qlad097&amp;#160;&lt;a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Science"></category><category term="analysis"></category></entry><entry><title>PA Well Water, Part 3</title><link href="https://swpease.github.io/pa-well-water-part-3.html" rel="alternate"></link><published>2025-06-09T00:00:00-07:00</published><updated>2025-06-18T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2025-06-09:/pa-well-water-part-3.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post continues from &lt;a href="https://swpease.github.io/pa-well-water-part-2.html"&gt;Part 2&lt;/a&gt; and covers the 
model's accounting for rurality.&lt;/p&gt;
&lt;p&gt;The authors write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;rural areas will have more domestic wells.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Which makes sense and is what I would presume, but their citation for said 
claim doesn't seem to say that(? I just did a search for …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post continues from &lt;a href="https://swpease.github.io/pa-well-water-part-2.html"&gt;Part 2&lt;/a&gt; and covers the 
model's accounting for rurality.&lt;/p&gt;
&lt;p&gt;The authors write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;rural areas will have more domestic wells.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Which makes sense and is what I would presume, but their citation for said 
claim doesn't seem to say that(? I just did a search for "rural" and "well" 
in the reference and didn't see anything). In any case, the &lt;a href="https://www.cdc.gov/environmental-health-services/php/water/private-water-public-health.html"&gt;CDC says so&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With that in mind, the authors don't seem to account for rurality as a 
confounder. They use county, which I suppose is related to rurality. I'll go 
over their model, and propose adding a rurality covariate in the form of 
census tract population density. I'll also discuss population density with 
respect to their chosen outcome, count of houses using domestic wells by 
census tract.&lt;/p&gt;
&lt;p&gt;At this point I think I should make it clear that I haven't read up on and 
am mostly unfamiliar with 
spatial data regression, so I may be doing obviously dumb things.&lt;/p&gt;
&lt;h2&gt;What is rurality?&lt;/h2&gt;
&lt;p&gt;It's a word we all know the meaning of, but there is a rather detailed set of 
criteria for 
what constitutes an "urban 
area", per the &lt;a href="https://www.govinfo.
gov/content/pkg/FR-2022-03-24/pdf/2022-06180.pdf"&gt;US Census Bureau&lt;/a&gt;. There are a lot of 
details, but the gist is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Census Bureau’s urban areas represent densely developed territory, and 
encompass residential, commercial, and other non-residential urban land 
uses. The boundaries of this urban footprint have been defined using 
measures based primarily on population counts and residential population 
density, and also on criteria that account for non-residential urban land 
uses, such as commercial, industrial, transportation, and open space that 
are part of the urban landscape.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Some Useful References&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://www.rural.pa.gov/home"&gt;Center for Rural Pennsylvania&lt;/a&gt; 
(apparently that's a thing) has some nice resources about rural PA, and in 
comparison to urban PA, for instance &lt;a href="https://www.rural.pa.gov/data/rural-quick-facts"&gt;Quick Facts&lt;/a&gt; is good to look through, and &lt;a href="https://www.rural.pa.gov/download.cfm?file=Resources/PDFs/Pennsylvania%27s%20Population%20is%20Declining%20Faster%20Than%20Expected%20Fact%20Sheet%20Web.pdf"&gt;A Report on PA 
Population Decline&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;DAGs&lt;/h1&gt;
&lt;p&gt;I think it's worthwhile to explicitly make some DAGs (using &lt;a href="https://www.dagitty.net"&gt;daggity&lt;/a&gt;) to 
get an 
idea of 
causality and confounding possibilities. For an introduction on the subject, 
the &lt;a href="https://github.
com/rmcelreath/stat_rethinking_2024/tree/main?tab=readme-ov-file"&gt;Statistical Rethinking&lt;/a&gt; lectures 
go over it (especially lectures 5 and 6). I'll build up from the simplest 
ones, piece by piece.&lt;/p&gt;
&lt;p&gt;As a reminder, the primary aim of the statistical analysis in this paper is 
to find "profiles" of SVI (as determined by clustering in a Bayesian profile 
regression) that are used to predict reliance on well water, and to 
characterize those profiles.&lt;/p&gt;
&lt;h2&gt;DAG 1: Adding County&lt;/h2&gt;
&lt;p&gt;The first DAG is just the predictor of interest (SVI cluster), the outcome 
(well water), and the confounder "county" that they use in the paper:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 1" src="https://swpease.github.io/images/science/well_water/dag1.png"&gt;&lt;/p&gt;
&lt;p&gt;To be explicit about some possible causal mechanisms, I'll propose that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;County =&amp;gt; SVI: characteristics of a county (e.g. laws) 
    can alter demographic profiles.&lt;/li&gt;
&lt;li&gt;County =&amp;gt; Well water: counties may have different regulations on water 
    rights.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;DAG 2: Adding Spatial Correlation&lt;/h2&gt;
&lt;p&gt;The second DAG adds in a spatial autocorellation term, as they did in the 
paper. See &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; and &lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. Per &lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;, the purpose of such a term is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;space is used as a proxy for any unmeasured variable; the common 
assumption is that areas which are close to each other are more similar 
than those further apart, suggesting that an additional source of 
correlation, namely spatial correlation needs to be accommodated in the models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this case, I'm thinking of underlying geology (aquifers, aquitards, etc.).
The DAG might look like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 2" src="https://swpease.github.io/images/science/well_water/dag2.png"&gt;&lt;/p&gt;
&lt;p&gt;Again, to be explicit about some possible causal mechanisms, I'll propose that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Location =&amp;gt; SVI: location, location, location. It can alter demographic profiles.&lt;/li&gt;
&lt;li&gt;Location =&amp;gt; Well water: The aforementioned geology issues (e.g. aquifers, 
  are you in a desert?).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;DAG 3 (Proposal): Adding Population Density&lt;/h2&gt;
&lt;p&gt;I propose adding in census tract population density as another confounder 
for which to 
control, acting as a measure of rurality. It probably correlates with 
county, but I think should do a 
better job of addressing rurality. I wonder how a model with both, one with 
only population density, and one with only county would compare.&lt;/p&gt;
&lt;p&gt;The DAG might look like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 3" src="https://swpease.github.io/images/science/well_water/dag3.png"&gt;&lt;/p&gt;
&lt;p&gt;Again, to be explicit about some possible causal mechanisms, I'll propose that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pop density =&amp;gt; SVI: People may move to/from urban/rural areas, yielding 
  distinct demographics. e.g. young people may want to move to the big city, 
  or old people may want a quiet retirement.&lt;/li&gt;
&lt;li&gt;Pop density =&amp;gt; Well water: As mentioned in the CDC reference. I suppose 
  less densely populated areas can more likely support well users. Though my 
  brother gets well water for watering his garden (but is on municipal 
  drinking water) in a Florida suburb.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Outcome Approximations&lt;/h1&gt;
&lt;p&gt;Some census tracts are fully or not at all served by municipal water sources.
Some are partially served by them. For these partial cases, we need to 
estimate values for their chosen outcome, count of houses using domestic wells by 
census tract. The authors state:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For each census tract we estimated the proportion of the population using 
a domestic well using the following assumptions: 1) census tracks without publicly supplied water
rely on domestic wells, 2) the proportional area served by a public water supply is the same as the
proportion of homes served by a PWS. To verify whether our assumptions were valid, we compared our
maps of proportion of homes served by private well water to population density maps of the state of
PA (S1 Fig).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, given the relationship between rurality and well water usage, this 
area-based approximation will likely overestimate well reliance. A reviewer 
of the manuscript notes this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I realize this is a pragmatic assumption and you later note that the areas with the public water supply may have a higher population density (whereas as the analysis assumes equal population density). I expect there would be a way to ‘weight’ the population in the areas inside and outside the public water supply so as to overcome this potential source of bias. e.g. I imagine there are reasonable shape files of population density that would enable this.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'll discuss it in more detail later, but thanks to &lt;sup id="fnref2:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;, I was able to find 
such population density maps thanks to &lt;a href="https://dataforgood.facebook.com/dfg/tools/high-resolution-population-density-maps"&gt;Facebook and CIESIN&lt;/a&gt;. I'm not sure why 
this information wasn't incorporated, given that Coker is an author on this 
paper. I guess it was considered a good enough approximation as per S1 Fig.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I've argued that population density should be included in their model as a 
rurality metric, acting to account for this potential confounding. I also am 
interested in seeing what, if any, difference using population density in 
estimating well usage might have.&lt;/p&gt;
&lt;p&gt;In the next post, I'll go over my main issue with the analysis: the (non)
use of Bayesian profile regression.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Liverani, S, Lavigne, A, Blangiardo, M, 2016. Modelling collinear and spatially
correlated data. Spatial and Spatio-temporal Epidemiology 18, 63–73. https://doi.org/10.1016/j.sste.2016.04.003.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Coker ES, Molitor J, Liverani S, Martin J, Maranzano P, Pontarollo N, et al. Bayesian profile regression to study the ecologic associations of correlated environmental exposures with excess mortality risk during the first year of the Covid-19 epidemic in lombardy, Italy. Environ Res 216(Pt 1):114484. Available from: https://europepmc.org/articles/PMC9547389 https:// doi.org/10.1016/j.envres.2022.114484 PMID: 36220446&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Science"></category><category term="analysis"></category></entry><entry><title>PA Well Water, Part 2</title><link href="https://swpease.github.io/pa-well-water-part-2.html" rel="alternate"></link><published>2025-06-01T00:00:00-07:00</published><updated>2025-06-01T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2025-06-01:/pa-well-water-part-2.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post continues from &lt;a href="https://swpease.github.io/pa-well-water-part-1.html"&gt;Part 1&lt;/a&gt; and covers the 
standardization of the 
covariates used for the 
clustering part of their model.&lt;/p&gt;
&lt;p&gt;I think it'll wind up pretty short. I had initially created some maps to 
show off, but decided that they didn't add anything substantial to the 
explanation.&lt;/p&gt;
&lt;h1&gt;My …&lt;/h1&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post continues from &lt;a href="https://swpease.github.io/pa-well-water-part-1.html"&gt;Part 1&lt;/a&gt; and covers the 
standardization of the 
covariates used for the 
clustering part of their model.&lt;/p&gt;
&lt;p&gt;I think it'll wind up pretty short. I had initially created some maps to 
show off, but decided that they didn't add anything substantial to the 
explanation.&lt;/p&gt;
&lt;h1&gt;My Gripe&lt;/h1&gt;
&lt;p&gt;The paper reads:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All values were rescaled into county-specific z-scores before being fit in 
the Bayesian Profile Regression model. This normalization of indicator 
data was done because each county has its own baseline level of values for 
the social vulnerability indicators. Additionally, these indicators vary 
not only between census tracts but within and between counties. If raw, unscaled values had been used, it would obscure actual inequalities between clusters that are occurring between counties. This similar rescaling was used by Coker et al. (2023) in assessing the spatial relationship between environmental exposure profiles and excess mortality risk due to COVID-19 in Lombardy, Italy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don't understand the logic for county-specific z-scores. What is special 
about these values on the county-relative scale? If a county has overall 
(for the state) low values, why does it make sense to have the largest value 
from that county pushed up to be with the overall (statewide) high values. 
The converse as well: a county with overall high values would have its 
smallest value pushed down to be with the overall low values.&lt;/p&gt;
&lt;p&gt;They say the rescaling is similar to Coker et al. (2023), but that paper 
doesn't mention any sort of grouping prior to z-scoring, stating:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We rescaled the respective values into Z-scores before fitting the Bayesian Profile Regression analysis&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;A Specific Case&lt;/h1&gt;
&lt;p&gt;Data source: &lt;a href="https://www.atsdr.cdc.
gov/place-health/php/svi/svi-data-documentation-download.html"&gt;PA Census Tracts SVI 2018&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I arbitrarily picked EP_UNEMP (The SVI's unemployment rate estimate) to 
examine. The table below shows Snyder County, the county most impacted by 
the z-scoring (i.e. standardization) for this 
variable. The county 
has 8 census tracts, hence 8 rows. The table has crap names because I needed to fit the table 
within my blog's margins. To specify the column definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;EP_UNEMP: unemployment rate estimate. Note if you're following 
    along that I replaced -999's with NA's.&lt;/li&gt;
&lt;li&gt;std_cnty: EP_UNEMP, standardized by county.&lt;ul&gt;
&lt;li&gt;dplyr code: &lt;code&gt;svi_unemp |&amp;gt; group_by(STCNTY) |&amp;gt; mutate(std_by_county = 
  as.vector(scale(EP_UNEMP))) |&amp;gt; ungroup()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;std_PA: EP_UNEMP, standardized by across all of PA&lt;ul&gt;
&lt;li&gt;dplyr code: &lt;code&gt;svi_unemp |&amp;gt; mutate(std_PA = as.vector(scale(EP_UNEMP)))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cnty_qntl: Empirical quantile of std_cnty&lt;ul&gt;
&lt;li&gt;dplyr code: &lt;code&gt;svi_unemp |&amp;gt; mutate(cnty_qntl = min_rank(std_cnty) / n())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PA_qntl: Empirical quantile of std_PA&lt;ul&gt;
&lt;li&gt;dplyr code: &lt;code&gt;svi_unemp |&amp;gt; mutate(PA_qntl = min_rank(std_PA) / n())&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;qntl_abs_diff: Absolute difference between PA_qntl and cnty_qntl&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'm not sure if there's a standard way to compute empirical quantiles, so 
maybe my numbers there are off.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: right;"&gt;EP_UNEMP&lt;/th&gt;
&lt;th style="text-align: right;"&gt;std_cnty&lt;/th&gt;
&lt;th style="text-align: right;"&gt;std_PA&lt;/th&gt;
&lt;th style="text-align: right;"&gt;cnty_qntl&lt;/th&gt;
&lt;th style="text-align: right;"&gt;PA_qntl&lt;/th&gt;
&lt;th style="text-align: right;"&gt;qntl_abs_diff&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;1.6&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-1.11&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.97&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.06&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.04&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;1.9&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.82&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.91&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.17&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.06&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;2.0&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.73&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.89&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.21&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.07&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;2.1&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.63&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.87&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.26&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.08&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;2.9&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.13&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.70&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.65&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.16&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;4.6&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1.75&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.36&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.94&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.42&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.52&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;3.3&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.51&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.62&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.77&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.22&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: right;"&gt;3.7&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.90&lt;/td&gt;
&lt;td style="text-align: right;"&gt;-0.54&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.84&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.28&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To me, the important thing to see is how the census tracts with unemployment 
rates that are high for Snyder County are still below the median for the 
state. This yields quite large differences in where they wind up, 
relative to 
the census tracts across all of PA, when you do county-specific 
standardization vs PA-wide standardization (where standardization = z-scoring).&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;That's about it. I could go on, but I think the idea is presented well 
enough. The researcher obviously thought about this and made the 
decision and effort to 
do this z-scoring, so maybe I'm just missing something and am the wrong one. &lt;/p&gt;</content><category term="Science"></category><category term="analysis"></category></entry><entry><title>PA Well Water, Part 1</title><link href="https://swpease.github.io/pa-well-water-part-1.html" rel="alternate"></link><published>2025-05-29T00:00:00-07:00</published><updated>2025-05-29T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2025-05-29:/pa-well-water-part-1.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I found a paper on PLOS Water:&lt;/p&gt;
&lt;p&gt;Wamsley M, Coker ES, Wilson RT, Henry K, Murphy HM (2024) Social 
vulnerability and exposure to private well water. PLOS Water 3(12): e0000303.
&lt;a href="https://doi.org/10.1371/journal.pwat.0000303"&gt;https://doi.org/10.1371/journal.pwat.0000303&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I can't do a better job than the authors of …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I found a paper on PLOS Water:&lt;/p&gt;
&lt;p&gt;Wamsley M, Coker ES, Wilson RT, Henry K, Murphy HM (2024) Social 
vulnerability and exposure to private well water. PLOS Water 3(12): e0000303.
&lt;a href="https://doi.org/10.1371/journal.pwat.0000303"&gt;https://doi.org/10.1371/journal.pwat.0000303&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I can't do a better job than the authors of summarizing the purpose, so 
here's the first few sentences of the abstract:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One quarter of the population of Pennsylvania relies on private domestic well water: two-fold greater than the US average. Private well owners are responsible for the maintenance and treatment of their water supply. Targeted interventions are needed to support these well owners to ensure they have access to safe drinking water, free of contaminants. To develop appropriate interventions, an understanding of the characteristics and social vulnerability of communities with high well water use is needed. The purpose of this study was to determine the spatial patterning of social vulnerability in Pennsylvania and assess the association between social vulnerability and private domestic wells using profile regression.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I think the paper is interesting, but I found a few issues with it that I 
wanted to go over. This has largely served as a great learning resource for 
me in a number of respects: mapping, Bayesian stats, epidemiology, stats in 
general, demographics... I'll go over the different issues I had over 
several posts. But first, what is "social vulnerability"?&lt;/p&gt;
&lt;h1&gt;Social Vulnerability / Social Vulnerability Index (SVI)&lt;/h1&gt;
&lt;p&gt;"Social vulnerability" has a few definitions that try to get at the same 
meaning, to varying success. I think that the Vermont public health 
department does the best job of explaining social vulnerability and the SVI: 
&lt;a href="https://www.healthvermont.
gov/sites/default/files/documents/2016/12/ENV_EPHT_SocialVulnerabilityIndex.
pdf"&gt;ref&lt;/a&gt;. The &lt;a href="https://www.atsdr.cdc.gov/place-health/php/svi/index.
html"&gt;ATSDR site&lt;/a&gt; has a definition that made me go, "What?", but its definition within 
its detailed &lt;a href="https://www.atsdr.cdc.
gov/place-health/php/svi/svi-data-documentation-download.html"&gt;documentation&lt;/a&gt; is much 
more sensible in my view. Vermont's 
definition is: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Social vulnerability refers to the resilience of communities when
responding to or recovering from threats to public health.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The ATSDR documentation's is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Every community must prepare for and respond to hazardous events, whether a natural disaster like a tornado or a disease outbreak, or an anthropogenic event such as a harmful chemical spill. The degree to which a community exhibits certain social conditions, including high poverty, low percentage of vehicle access, or crowded households, among others, may affect that community’s ability to prevent human suffering and financial loss in the event of a disaster. These factors describe a community’s social vulnerability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Similarly, Social Vulnerability Index (SVI) per Vermont is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a planning tool to evaluate the relative social vulnerability across the 
state. It can be used if there is a disease outbreak or in the event of an 
emergency—either natural or human-caused—to identify populations that may 
need more help.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And for ATSDR is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;to help public health officials and emergency response planners identify 
and map the communities that will most likely need support before, during, 
and after a hazardous event.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In general, I thought the Vermont reference was the best, both for being 
concise, and importantly for highlighting the SVI's limitations. I'd read 
that if I were you.&lt;/p&gt;
&lt;h2&gt;A Brief Aside&lt;/h2&gt;
&lt;p&gt;Going to the ATSDR website shows a banner:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 1" src="https://swpease.github.io/images/science/well_water/ATSDR_site.png"&gt;&lt;/p&gt;
&lt;p&gt;I don't really go on government sites much, so I was astounded.&lt;/p&gt;
&lt;h1&gt;My Issues With This Paper&lt;/h1&gt;
&lt;h2&gt;Re Its Premise&lt;/h2&gt;
&lt;p&gt;I'm actually fine with its premise. A few relevant excerpts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Since upkeep of private water supplies falls completely on the well-owner, community-level and individual-level social deprivation/vulnerability is likely to be an important determinant of whether well-owners can keep up with the requirements to ensure safe drinking water.&lt;/p&gt;
&lt;p&gt;the socioeconomic and demographic characteristics of the population using domestic wells (that are excluded from public water supplies) are not well described.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, I do wonder if in a real situation you might not just ask the 
regional/local government for information on the local well users, giving 
better information than this study could? The aforementioned Vermont user's 
guide seems to agree, referring to the SVI as a "first step" in screening 
populations and adding:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Local information might be more accurate than these
estimates and should always be considered if it is available.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you want to know more, just read the paper's intro.&lt;/p&gt;
&lt;h2&gt;Re Its Conclusions&lt;/h2&gt;
&lt;p&gt;Their analysis indicated that well users were across the gamut of SVI, 
yielding a fairly broad conclusion. Some groups had high SVI (well, I'll argue 
that point later), some low. They say that the high SVI groups comprised a 
large population (1.1 million, ~1/3 of PA well users), but do not provide a 
list of populations for each group, which would have been nice.&lt;/p&gt;
&lt;p&gt;A few instances of trying to work around this nebulousness:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;it seems reasonable to focus education interventions on all high private well use areas&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well I could've told you that.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The knowledge gained from our analysis in PA informs us that interventions and education campaigns need to be targeted towards homeowners (and some renters), and populations that are more socially vulnerable (have a low level of education, with an annual income less than $30k, are elderly or have children) as well as those that are less socially vulnerable (those with higher levels of education, an annual income over $30k, and are not a sensitive age).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, that's a pretty wide net.&lt;/p&gt;
&lt;h2&gt;Re Its Methods&lt;/h2&gt;
&lt;p&gt;This is where problems arose, I think. I want to be clear that I am not 
trying to be mean, that I might be wrong in some respects in my criticisms, 
and that I've been there as a grad student.&lt;/p&gt;
&lt;p&gt;My first doubts arose when looking at the regression formula in the section 
"Multilevel risk model". It is a mess. The indexing is wrong and there is a 
dummy variable trap. I think there might be other issues, but it could just 
be unfamiliar notation.&lt;/p&gt;
&lt;p&gt;I have three primary issues:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;They z-scored by county.&lt;/li&gt;
&lt;li&gt;They didn't include a good "rurality" covariate. &lt;/li&gt;
&lt;li&gt;They didn't actually do Bayesian Profile Regression.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I'll go over each of these in separate posts. I think it's good to have a 
grasp on the "units" in this study: census tracts.&lt;/p&gt;
&lt;h1&gt;Census Tracts&lt;/h1&gt;
&lt;p&gt;This paper references a data source as the US Census Bureau's TIGER/Line 
Shapefiles &lt;a href="https://www.census.
gov/geographies/mapping-files/time-series/geo/tiger-line-file.html"&gt;here&lt;/a&gt;. If you 
download a Shapefile, the dataset's abstract reads in part (emphasis mine):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Census tracts are small, relatively permanent statistical subdivisions of 
a county or equivalent entity, and were defined by local participants as 
part of the Census 2000 Participant Statistical Areas Program (PSAP). The 
Census Bureau delineated the census tracts in situations where no local 
participant existed or where all the potential participants declined to participate. The primary purpose of census tracts is to provide a stable set of geographic units for the presentation of census data and comparison back to previous decennial censuses. Census tracts generally have a population size between 1,500 and 8,000 people, with an optimum size of 4,000 people. &lt;strong&gt;When first delineated, census tracts were designed to be homogeneous with respect to population characteristics, economic status, and living conditions&lt;/strong&gt;. The spatial size of census tracts varies widely depending on the density of settlement. Physical changes in street patterns caused by highway construction, new development, etc. may require boundary revisions before a census. In addition, census tracts occasionally are split due to population growth, or combined as a result of substantial population decline. Census tract boundaries generally follow visible and identifiable features. They may follow legal boundaries such as minor civil division (MCD) or incorporated place boundaries in some States and situations to allow for census tract-to-governmental unit relationships where the governmental boundaries tend to remain unchanged between censuses. State and county boundaries are always census tract boundaries in the standard census geographic hierarchy. In a few rare instances, a census tract may consist of noncontiguous areas. These noncontiguous areas may occur where the census tracts are coextensive with all or parts of legal entities that are themselves noncontiguous. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Also, looking at the &lt;a href="https://www.census.
gov/programs-surveys/geography/technical-documentation/complete-technical
-documentation/tiger-geo-line.html"&gt;technical documentation&lt;/a&gt; I feel is helpful. For instance, the 
&lt;a href="https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2010/TGRSHP10SF1.pdf"&gt;2010 documentation&lt;/a&gt; 
provides a visual hierarchy of census entities in its Figure 1.&lt;/p&gt;
&lt;h1&gt;Simple Issues&lt;/h1&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;h3&gt;Cluster 14&lt;/h3&gt;
&lt;p&gt;Cluster 14 was classified by the author as having "suggested social 
vulnerability". However, per Fig 5, for the 10 social vulnerability 
covariates, the median quartiles for Cluster 14 were comprised of 5 in 
quartile 2 and 5 in quartile 3, which should indicate a remarkably 
middle-of-the-road social vulnerability level. You might be thinking, "well, 
you have to see the full distributions to know for sure." Well, that's what 
I thought, too. The odd thing is, full distributions are a built-in 
visualization function provided by the R package that I'm guessing they used 
for their analysis.&lt;/p&gt;
&lt;h2&gt;Nitpicks&lt;/h2&gt;
&lt;p&gt;Beyond the aforementioned formula confusion, I found some other errors.&lt;/p&gt;
&lt;h3&gt;Table 1&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Columns are mis-aligned.&lt;/li&gt;
&lt;li&gt;Cluster 1's Poisson model estimate should be 0.28, not 1.28.&lt;/li&gt;
&lt;li&gt;Cluster 12's mean # households should be 1140 or 1149 I'm guessing. They 
   probably fat-fingered 90.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Fig 6&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The y-axis is cut-off.&lt;/li&gt;
&lt;li&gt;"The random effects due to county are shown in Fig 6." No, they aren't.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Future Writing&lt;/h1&gt;
&lt;p&gt;I'll write separate posts at least on each of my 3 beefs(?) with this paper, 
and want to write some more about Bayesian Profile Regression afterwards.&lt;/p&gt;</content><category term="Science"></category><category term="analysis"></category></entry><entry><title>Sand Crabs Eating Bits of Plastic String (Part 2)</title><link href="https://swpease.github.io/sand-crabs-eating-bits-of-plastic-string-part-2.html" rel="alternate"></link><published>2024-12-17T00:00:00-08:00</published><updated>2024-12-17T00:00:00-08:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2024-12-17:/sand-crabs-eating-bits-of-plastic-string-part-2.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In the &lt;a href="https://swpease.github.io/sand-crabs-eating-bits-of-plastic-string.html"&gt;previous post&lt;/a&gt; on sand crabs, I 
looked at the mortality of the treatments (exposed to environmentally 
relevant concentrations of microplastic 
fibers)
vs controls. Now, I want to look at another outcome that they looked at: egg 
development.&lt;/p&gt;
&lt;p&gt;Specifically, looking at starting vs ending egg development stages. This …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In the &lt;a href="https://swpease.github.io/sand-crabs-eating-bits-of-plastic-string.html"&gt;previous post&lt;/a&gt; on sand crabs, I 
looked at the mortality of the treatments (exposed to environmentally 
relevant concentrations of microplastic 
fibers)
vs controls. Now, I want to look at another outcome that they looked at: egg 
development.&lt;/p&gt;
&lt;p&gt;Specifically, looking at starting vs ending egg development stages. This 
endpoint is.. much trickier to think about. For starters,
it would 
help 
if I knew much about sand crab reproduction, or developmental biology in 
general; it was always a bit of a snooze for me. I mean, I thought the 
reproductive physiology course I took was interesting, but that's different. &lt;/p&gt;
&lt;p&gt;The authors explain:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The selected experimental time frame (71 d) allowed for two full embryonic 
development cycles as E. analoga has an incubation cycle of 29–32 days.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don't really get how this works. Why is one full cycle insufficient? Are 
they expecting slower development in the treatment crabs? Is this just my 
lack of 
understanding of developmental biology?&lt;/p&gt;
&lt;p&gt;Adding onto that, there is another observed variable, "# of days with viable 
eggs", which I don't really understand with respect to the start/end egg 
development stages. Do the eggs become inviable after completed development? 
Are there commonly miscarriages (or the egg equivalent)? I guess I should 
just treat this as an exercise in Bayesian stats. It's not like I'll have 
any idea on how to make a useful prior, anyway. &lt;/p&gt;
&lt;h1&gt;Data Discrepancies (Again)&lt;/h1&gt;
&lt;p&gt;I am using the GitHub data, because the Zenodo data for egg development 
doesn't make sense. The GitHub data also has several issues, but at least 
mostly makes sense. For the GitHub data, there are 3 control crabs missing 
and 4 treatment crabs 
missing, 
which yields 29 control crabs and 28 treatment crabs.&lt;/p&gt;
&lt;p&gt;I made a couple of bar charts as descriptive statistics-type things, which 
let me notice that Treatment Crab #5 has wrong data for its egg development 
columns. I was going to just omit it, but I see that there are two other crabs 
for 
which the "# of egg stages(start to finish)" column — which should presumably 
validate the difference between start and end egg stages — is off by one, so 
I think that Treatment Crab #5's end stage was "Z", not 2, plus its "# of egg 
stages(start to finish)" is off by two. This supports by "bad handwriting" 
hypothesis from the previous post. I know I started putting lines through my 
Z's after doing math where some formula creator (I'm guessing with good 
handwriting) 
decided to use Z's and 2's all over the place. Stupid mxn. Stupid ijk. At 
least I'm &lt;a href="https://betterexplained.
com/articles/linear-algebra-guide/"&gt;not alone&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, there is a column "Reach Larval Stage(9+) Y/N", which has six 
discrepancies with "Egg Stage end of exp". &lt;/p&gt;
&lt;p&gt;Also, there are seven cases of "# of days with viable eggs" exceeding "# of 
days alive".&lt;/p&gt;
&lt;p&gt;I guess these data discrepancies further support my thinking of, "Don't take 
the results of my analysis too seriously.".&lt;/p&gt;
&lt;h1&gt;Descriptive Plots&lt;/h1&gt;
&lt;p&gt;Here's what the distribution of egg development stages looked like at the 
start and end of the experiment:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 1" src="https://swpease.github.io/images/science/sand_crabs/eggs/starting_stage_hist.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 2" src="https://swpease.github.io/images/science/sand_crabs/eggs/ending_stage_hist.png"&gt;&lt;/p&gt;
&lt;p&gt;And how many stages were progressed, based on starting stage, and split out 
by whether the crab died before the end of experiment or not.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 3" src="https://swpease.github.io/images/science/sand_crabs/eggs/stages_progressed.png"&gt;&lt;/p&gt;
&lt;p&gt;I was on the fence about splitting out the above figure by died/lived. I 
decided to, because with them combined, my first question was, "Did those 
starting stage 10 treatment crabs with no development progression die early?
". I suppose I could've coded died/lived by shape, too.&lt;/p&gt;
&lt;h1&gt;The Modeling&lt;/h1&gt;
&lt;p&gt;Firstly, an explanatory note re what follows. I did the modeling months 
ago, but had wanted to extend the model to incorporate censoring. Censored 
discrete distributions were(/are) not implemented in PyMC, as I discovered. 
One of the maintainers (Ricardo) fixed that problem within a couple of days 
of my bringing it up. However, said fix has been &lt;a href="https://github.com/pymc-devs/pymc/pull/7662"&gt;sitting in pull request 
purgatory&lt;/a&gt; for months now. Oh well. The point is, I'm revisiting this 
analysis months after having done it, so I am probably forgetting some 
nuances of decisions/explanations.&lt;/p&gt;
&lt;p&gt;So just looking at the above plots, I was expecting the model output to show 
minimal differences between treatments and controls, for whichever model I 
decided to use.&lt;/p&gt;
&lt;p&gt;I opted to use a proportional odds (ordered logit) model. I don't think it's 
entirely 
appropriate, and looking for an alternative model for ordinal data led me to 
discover the &lt;a href="https://www3.nd.
edu/~rwilliam/gologit2/UnderStandingGologit2016.pdf"&gt;generalized ordered logit model&lt;/a&gt;, but it seemed like the best 
option of any model options I could find. As it is, I don't really think 
that the appropriateness of a proportional odds model is 
assessable by humans without using the Brant Test. Or maybe I just haven't 
become one with the log-odds yet.&lt;/p&gt;
&lt;p&gt;The DAG of this model made me go "Woof!". On paper, its description is 
pretty simple: an ordered categorical outcome with an ordered categorical 
predictor and a categorical predictor. That is: final egg stage outcome with 
starting egg stage and treatment group predictors. Well, everything 
right/down of beta_trt is derived stuff (Stan's "generated quantities" block),
so 
I guess you can ignore that part.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 4" src="https://swpease.github.io/images/science/sand_crabs/eggs/DAG.png"&gt;&lt;/p&gt;
&lt;p&gt;And here is the PyMC model that produced that thing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;N_cutpoints&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Egg Stage end of exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;N_deltas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Egg Stage at Beginning of exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;coords&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;grp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;t&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;end_stage&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Egg Stage end of exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;start_stage&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Egg Stage at Beginning of exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;cutpoint&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_cutpoints&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;model4&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Data&lt;/span&gt;
    &lt;span class="n"&gt;y_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y_data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Egg Stage end of exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;codes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;obs_id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;start_stage_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;start_stage_data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Egg Stage at Beginning of exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;codes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;obs_id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;grp_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;grp_data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;c_or_t_grp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;obs_id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Priors&lt;/span&gt;
    &lt;span class="n"&gt;cutpoints&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cutpoints&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_cutpoints&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                        &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                    &lt;span class="c1"&gt;#   initval=(np.arange(N_cutpoints) / 2) - 2.5,&lt;/span&gt;
                        &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;distributions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ordered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_cutpoints&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;beta_trt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beta_trt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;beta_start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beta_start&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dirichlet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;delta&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_deltas&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;N_deltas&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;delta_j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;delta_j&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;delta&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="n"&gt;delta_j_cumulative&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;delta_j_cumulative&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;delta_j&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# params&lt;/span&gt;
    &lt;span class="n"&gt;phi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;phi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                           &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_start&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;delta_j_cumulative&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;start_stage_data&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; 
                            &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta_trt&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;grp_data&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                            &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;obs_id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# lik&lt;/span&gt;
    &lt;span class="n"&gt;y_obs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OrderedLogistic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y_obs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                               &lt;span class="n"&gt;eta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                               &lt;span class="n"&gt;cutpoints&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cutpoints&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                               &lt;span class="n"&gt;observed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                               &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;grp_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="c1"&gt;# derived params&lt;/span&gt;
    &lt;span class="n"&gt;cutpoints_col&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cutpoints_col&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cutpoints&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="n"&gt;bc_delta_j_cumulative&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bc_delta_j_cumulative&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                        &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;broadcast_to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;delta_j_cumulative&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_cutpoints&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_deltas&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="n"&gt;cutpoints_by_grp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cutpoints_by_grp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                        &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
                                            &lt;span class="n"&gt;cutpoints_col&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bc_delta_j_cumulative&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;beta_start&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;  &lt;span class="c1"&gt;# phi = 0&lt;/span&gt;
                                            &lt;span class="n"&gt;cutpoints_col&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;bc_delta_j_cumulative&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;beta_start&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta_trt&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;  &lt;span class="c1"&gt;# phi = beta_trt&lt;/span&gt;
                                        &lt;span class="p"&gt;],&lt;/span&gt;
                                        &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                        &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cutpoint&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;start_stage&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;grp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# derived vars&lt;/span&gt;
    &lt;span class="n"&gt;cum_probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cum_probs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;invlogit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cutpoints_by_grp&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cutpoint&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;start_stage&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;grp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;probs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                             &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;cum_probs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;
                             &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;cum_probs&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
                             &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;end_stage&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;start_stage&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;grp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I had quite a time making myself confident that the indexing tetris required 
for &lt;code&gt;probs&lt;/code&gt; wound up correct.&lt;/p&gt;
&lt;h2&gt;Output&lt;/h2&gt;
&lt;p&gt;Assessing the output of this model was a bit of a chore. I looked at it from 
a couple of different perspectives:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Posterior predictions&lt;/li&gt;
&lt;li&gt;The distributions of the &lt;code&gt;probs&lt;/code&gt; in a forest plot.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first view looks neater, while the second provides HDI's.&lt;/p&gt;
&lt;p&gt;First, the posterior predictions. This graph could be cleaner, but the lack 
of any apparent difference between treatments and controls kind of took the 
wind out of my sails, at least to the extent of fixing the 
indexing and re-looking-up how to 
customize legends in seaborn/matplotlib. The bins are split by control = 0 
and treatment = 1 (as if it matters), and ordered by ending stage (0 = stage 
1, ..., 9 = stage 10, 10 = stage Z). I suppose the main wrong thing with 
this model is how the probability of final stages earlier than starting 
stages is non-zero.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 5" src="https://swpease.github.io/images/science/sand_crabs/eggs/posterior_preds.png"&gt;&lt;/p&gt;
&lt;p&gt;Second, the posterior probabilities arranged as ["end_stage", "start_stage", 
"grp"]. I think it's a bit interesting how the &lt;code&gt;end_stage&lt;/code&gt; of zygote 
monotonically increases with &lt;code&gt;start_stage&lt;/code&gt;, and non-zygote end stages kind 
of do a wave pattern indicating the most highly expected development of maybe 
5-ish stages (e.g. the [9, x, x] portion of the graph):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 5" src="https://swpease.github.io/images/science/sand_crabs/eggs/posterior_probs.png"&gt;&lt;/p&gt;
&lt;p&gt;When it comes down to it, there's not a lot of data to go off of, though, 
and I don't think all this modeling work does a whole lot more than what 
could be eyeballed based on the descriptive stats/plots. &lt;/p&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;Keeping in mind the caveats of "not my experiment" and "I'm not an invert 
zoologist", I think that this experiment doesn't show any meaningful 
difference between treatments and controls in terms of egg development.&lt;/p&gt;
&lt;p&gt;Probably the most valuable thing to come out of this exercise was the 
(pending) enhancement to PyMC's capabilities. I figure that even if I'd 
gotten to incorporate censoring into the model, all it would've done 
would've been to make the HDI's wider and maybe push some point estimates 
around.&lt;/p&gt;
&lt;h1&gt;Discussion (but not what you're expecting)&lt;/h1&gt;
&lt;p&gt;I was thinking about the survival analysis from part 1, and it occurred to 
me that these "environmentally relevant concentrations" are if anything 
&lt;em&gt;low&lt;/em&gt;, because in the wild, there is a continuously replenishing supply of 
microplastics for the crabs to eat, while this experiment capped their 
exposure: if one of the three plastics was eaten, then the ambient 
concentration was now two per jar, until the new batch was introduced. So 
the crabs in the wild should be hurting even more than this experiment 
suggests. Have people noticed sand crab population shifts? I feel like as a 
kid there were tons around at the beach, but even just a decade later they 
were much rarer, though that's a pretty flimsy bit of anecdotal evidence 
(for instance, I 
certainly went to the shore much less often as I got older).&lt;/p&gt;</content><category term="Science"></category><category term="analysis"></category></entry><entry><title>Randomized Quantile Residuals</title><link href="https://swpease.github.io/randomized-quantile-residuals.html" rel="alternate"></link><published>2024-12-11T00:00:00-08:00</published><updated>2024-12-11T00:00:00-08:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2024-12-11:/randomized-quantile-residuals.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post should hopefully be helpful for people like me who aren't 
statisticians, but can work with R decently.&lt;/p&gt;
&lt;p&gt;While working with count data, I encountered a new type of residual 
(apparently there's different types) 
called the "randomized quantile residual" (RQR).
RQRs essentially convert your residuals such that you …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post should hopefully be helpful for people like me who aren't 
statisticians, but can work with R decently.&lt;/p&gt;
&lt;p&gt;While working with count data, I encountered a new type of residual 
(apparently there's different types) 
called the "randomized quantile residual" (RQR).
RQRs essentially convert your residuals such that you can interpret them 
just like plain old residuals from a regular old linear model.&lt;/p&gt;
&lt;p&gt;I tried reading the &lt;a href="https://www.researchgate.
net/publication/2647151_Randomized_Quantile_Residuals"&gt;original paper&lt;/a&gt; and was left confused 
by the math notation. So I found a &lt;a href="https://pubmed.ncbi.nlm.
nih.gov/32611379/"&gt;second paper&lt;/a&gt; yet remained unenlightened. So I finally just looked at 
the &lt;a href="https://rdrr.io/cran/statmod/src/R/qres.R"&gt;implementation&lt;/a&gt; and thought, 
"That's it?" So I understood how it worked, but not &lt;em&gt;why&lt;/em&gt; it worked. Until 
now (I think), when I played around with distributions in R some.&lt;/p&gt;
&lt;h1&gt;The Underlying Principle&lt;/h1&gt;
&lt;p&gt;What percent of the time do you expect to see something in the bottom 0-10th 
percentile? The 80-90th percentile? The 30-60th percentile? If you answered 
"10. 10. 30.", congratulations, you understand the underlying principle. Put 
another way, &lt;strong&gt;if you bin your data into &lt;a href="https://en.wikipedia.
org/wiki/Quantile"&gt;quantiles&lt;/a&gt;, 
each quantile should have about the same number of observations in it.&lt;/strong&gt; In 
retrospect, this is basic stats, but it's the type of thing you forget about 
after years of it never coming up. &lt;/p&gt;
&lt;p&gt;In 
small enough bins and infinite data, you should wind up with a &lt;strong&gt;Uniform 
distribution.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Converting to Uniform&lt;/h2&gt;
&lt;p&gt;Here are a couple of examples of going from some continuous distribution to 
these equal bins.&lt;/p&gt;
&lt;p&gt;First, simulated Exponentially distributed data, binned into however many bins 
&lt;code&gt;hist&lt;/code&gt; uses by default. If you forget / don't know &lt;code&gt;rexp&lt;/code&gt; vs &lt;code&gt;pexp&lt;/code&gt; vs 
&lt;code&gt;qexp&lt;/code&gt;, etc., &lt;a href="https://www.reddit.
com/r/RStudio/comments/1bwthox/can_anyone_please_explain_to_me_the_difference
/"&gt;here&lt;/a&gt; is a useful explanation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;simdat = rexp(n=10000, rate = 3)
hist(pexp(simdat, rate = 3))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Figure 1" src="https://swpease.github.io/images/statistics/rqrs/basic_uniform_ex.png"&gt;&lt;/p&gt;
&lt;p&gt;Second, simulated Beta distributed data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;simdat = rbeta(n=10000, 8, 13)
hist(pbeta(simdat, 8, 13))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Figure 2" src="https://swpease.github.io/images/statistics/rqrs/basic_uniform_ex2.png"&gt;&lt;/p&gt;
&lt;h2&gt;Converting Back&lt;/h2&gt;
&lt;p&gt;Once you're at the uniform distribution, you can go back to your starting 
distribution, or any other continuous distribution (it's not like 
the uniform distribution knows where it came from). &lt;strong&gt;The second option is 
used in RQRs: they're converted to Normal from some unfriendly distribution 
(e.g. Poisson) via the Uniform.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These two histograms are equal (not shown):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;simdat = rbeta(n=10000, 8, 3)
hist(simdat)
hist(qbeta(pbeta(simdat, 8, 3), 8, 3))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Going to a different distribution:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;simdat = rbeta(n=10000, 8, 3)
simdat2 = rexp(n=10000, rate = 3)
hist(simdat2)
hist(qexp(pbeta(simdat, 8, 3), rate = 3))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Figure 3" src="https://swpease.github.io/images/statistics/rqrs/exp_rate_3.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 4" src="https://swpease.github.io/images/statistics/rqrs/beta_to_exp.png"&gt;&lt;/p&gt;
&lt;h1&gt;Misspecification&lt;/h1&gt;
&lt;p&gt;This uniformness only happens if your data is accurately represented by the 
CDF you choose (i.e. the &lt;code&gt;p&lt;/code&gt; functions in R). For instance, in the first 
example, my data came from an Exponential distribution with a rate of 3, and 
the CDF I used just copied over that knowledge. But what if it was different?&lt;/p&gt;
&lt;p&gt;What if our CDF was off?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;simdat = rexp(n=10000, rate = 3)
hist(pexp(simdat, rate = 4))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Figure 5" src="https://swpease.github.io/images/statistics/rqrs/exp_misspec1.png"&gt;&lt;/p&gt;
&lt;p&gt;Not so good.&lt;/p&gt;
&lt;p&gt;What if we picked an entire wrong distribution?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;simdat = rexp(n=10000, rate = 3)
hist(pnorm(simdat))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Figure 6" src="https://swpease.github.io/images/statistics/rqrs/exp_misspec2.png"&gt;&lt;/p&gt;
&lt;p&gt;Woof.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The RQRs pick up this non-uniformity so your residuals will look off when 
you convert them to Normal.&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;What About Discrete Data?&lt;/h1&gt;
&lt;p&gt;The main purpose of using RQRs! Let's see what happens when we do that CDF 
conversion with Poisson data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ggplot2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;simdat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rpois&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;lambda&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;cdf_dat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;ppois&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;simdat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;lambda&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nx"&gt;plot_dat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cdf_dat&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//stackoverflow.com/questions/3418128/how-to-convert-a-factor-to-integer-numeric-without-loss-of-information&lt;/span&gt;
&lt;span class="nx"&gt;plot_dat&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nx"&gt;cdf_dat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;numeric&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;plot_dat&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nx"&gt;cdf_dat&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="nx"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;plot_dat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;cdf_dat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;Freq&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;geom_point&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;geom_segment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;cdf_dat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;xend&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;cdf_dat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;yend&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;Freq&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Figure 7" src="https://swpease.github.io/images/statistics/rqrs/pois_cdf_hist.png"&gt;&lt;/p&gt;
&lt;p&gt;That's not flat. But, the counts of each bin are proportional to the 
proportion of the [0,1] range they encompass. &lt;strong&gt;RQRs work around this by 
taking a random number between the CDF at the observed value and CDF at one 
less than the 
observed value.&lt;/strong&gt; For instance, if you observe 1 (corresponding to the line at 
~0.2), which should happen about (0.2 - 0.05) proportion of the time (note 
the count (Freq) is 1500, i.e. 15% of the total 10,000 samples drawn. Also, 
0.05 
is roughly the CDF value for observed = 0, the leftmost line), you get a random 
number between 
roughly 0.05-0.2. So, ~15% of the time you get randomly selected values in this 
~15% range.&lt;/p&gt;
&lt;p&gt;Again, this is predicated on a Poisson distribution being appropriate for 
your data, and deviations will show up via non-Normal looking RQRs.&lt;/p&gt;
&lt;p&gt;Here is some code that might help to play around with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;tibble&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;
&lt;span class="nx"&gt;lambda&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="nx"&gt;simdat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;tibble&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;draw&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rpois&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;lambda&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;quantile&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;ppois&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;lambda&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;quantile_q_minus_1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;ppois&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;draw&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;lambda&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;quantile_diff&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;quantile&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;quantile_q_minus_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;random_quantile&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;runif&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;quantile_q_minus_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;corresponding_normal_val&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;qnorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;random_quantile&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Statistics"></category><category term="statistics"></category></entry><entry><title>Sand Crabs Eating Bits of Plastic String</title><link href="https://swpease.github.io/sand-crabs-eating-bits-of-plastic-string.html" rel="alternate"></link><published>2024-10-15T00:00:00-07:00</published><updated>2024-10-15T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2024-10-15:/sand-crabs-eating-bits-of-plastic-string.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I read a paper, &lt;a href="https://aslopubs.
onlinelibrary.wiley.com/doi/10.1002/lol2.10137"&gt;"Effects of environmentally relevant concentrations of 
microplastic fibers on Pacific mole crab (Emerita analoga) mortality and
reproduction." Horn DA; Granek EF; Steele CL. 2019.&lt;/a&gt; as part of reading up on 
stuff for a job I applied to. I had been learning how to use Bayesian …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I read a paper, &lt;a href="https://aslopubs.
onlinelibrary.wiley.com/doi/10.1002/lol2.10137"&gt;"Effects of environmentally relevant concentrations of 
microplastic fibers on Pacific mole crab (Emerita analoga) mortality and
reproduction." Horn DA; Granek EF; Steele CL. 2019.&lt;/a&gt; as part of reading up on 
stuff for a job I applied to. I had been learning how to use Bayesian 
methods, so this seemed like a good way to try our using real data from the 
wild. I'll link throughout to files in a repository that shows my work.&lt;/p&gt;
&lt;p&gt;The gist of the experiment was they set up 64 jars of sand + sea water, with a 
crab in 
each.
32 
were subject to the addition of three plastic fibers (1mm long) every four 
days, for up to 71 days. Dead crabs were frozen for later analysis.&lt;/p&gt;
&lt;p&gt;I have been a bit confused by their analysis methods. They use a linear 
mixed effects model, which is all well and good, but there seems to be a few 
things that don't make sense.&lt;/p&gt;
&lt;h1&gt;Data Discrepancies&lt;/h1&gt;
&lt;p&gt;The paper provides the data at two sites: &lt;a href="https://zenodo.
org/records/3564736"&gt;Zenodo&lt;/a&gt; and &lt;a href="https://github.
com/cwgrldotty/Sand-Crab-PSU-Data"&gt;GitHub&lt;/a&gt;. The problem is they don't line up: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The GitHub data is missing rows.&lt;/li&gt;
&lt;li&gt;The Zenodo data is missing columns.&lt;/li&gt;
&lt;li&gt;Some of the data is different:&lt;ul&gt;
&lt;li&gt;environmental plastics present&lt;/li&gt;
&lt;li&gt;egg stage at end of experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think that the "egg stage at end of experiment" data is correct for GitHub,
and wrong for Zenodo, because there is a "# of egg stages" column as well 
that on the Zenodo dataset sometimes exceeds "egg stage at end of 
experiment", which would mean starting at negative egg stages.&lt;/p&gt;
&lt;p&gt;I don't think they used junk data for their analyses; I just think the 
uploads are wonky. Well, there is one possible data error: I think crab 15's 
days alive number should be 21, not 71, based on the 
recorded date of death. As someone with terrible handwriting, I could 
totally see mixing up a 2 and a 7. &lt;/p&gt;
&lt;p&gt;I made the presumed correction to crab 15 for my analyses. I used the Zenodo 
data for 
survival times because it wasn't missing crabs.&lt;/p&gt;
&lt;h1&gt;Survival Curve&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.
com/swpease/sand-crabs/blob/master/sand_crab_mortality.Rmd"&gt;R Markdown Notebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The lack of a survival curve surprised me. Instead, they have a boxplot that 
I can't figure out. It doesn't match the data. My guess is that it's the 
fitted survival times post-linear mixed effects model, i.e. it's the 
shrunken (via partial pooling) survival times. As it is, here is the 
survival curves, with 50% CI's:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 1" src="https://swpease.github.io/images/science/sand_crabs/survival_curves.png"&gt;&lt;/p&gt;
&lt;p&gt;I used a log-rank test to compare the treatment vs control groups' survival 
curves. The p-value of the (presumed) corrected data was 0.10, which is higher 
than the publication's value of 
0.03 for their assessment of difference in mortality. Though,
the (presumed) wrong data yielded a p-value for 
the log-rank test of 0.05, which is obviously much closer. &lt;/p&gt;
&lt;p&gt;10 vs 17 
deaths does seem quite a bit different. I always find myself mentally replacing 
something below some subconsciously-determined p-value threshold as 
"definitely", but 0.1 just means, well there's evidence. Unless I want it 
&lt;em&gt;not&lt;/em&gt; to be true, in which case I think, "yeah it's totally a case of that 
10%". I do 
think it's slightly interesting 
that 
the curves are the same for the first three weeks. I wonder if there's 
anything to that. Maybe it lends credence to the authors' idea that the 
mechanism of harm is the dyes in the plastics. I could imagine that 
the body burden of dye is 
steadily increasing over time.&lt;/p&gt;
&lt;h1&gt;"Number of PP Fibers Internalized"&lt;/h1&gt;
&lt;p&gt;I don't like this name for this variable. At the end of the experiment 
(crabs that died before the end were frozen), they 
dissolved the crabs and counted the number of plastic fibers they filtered 
out of that. That gives you the body burden at death/censoring, not 
necessarily the total number of fibers they internalized. What if they 
could expel the fibers somehow? Also, what if some of the fibers were stuck 
to their outside? I'm guessing they washed them or something to account for 
this, since they were meticulous in preventing contamination at all the 
other phases, but didn't mention anything. Or maybe it's a non-issue.&lt;/p&gt;
&lt;p&gt;This variable is used for regressions to explain mortality, the number of 
days that a crab held live/viable eggs, and other outcomes. I don't think 
it's an accurate representation of what it claims to be. Here is a graph of 
number of fibers collected out of each crab at the end of the experiment vs 
when they died:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 2" src="https://swpease.github.io/images/science/sand_crabs/body_burden.png"&gt;&lt;/p&gt;
&lt;p&gt;The horizontal axis has ticks every 20 because I couldn't figure out how to get 
ggplot to make it how I wanted in the amount of time it took for me to get 
mad at The Grammar of Graphics&amp;trade;.&lt;/p&gt;
&lt;p&gt;Anyway, it seems highly unlikely in looking at this graph that every fiber a 
crab intakes stays in it until death. The number never exceeds five, in 
spite of reaching five in one crab after one third of the experiment's 
duration, and three in one crab after nine days.&lt;/p&gt;
&lt;p&gt;I tried a statistical test to get at this being the case, by looking for an 
association between exposure duration (i.e. days alive in the experiment) and 
number of fibers internalized. I 
used a Poisson regression on &lt;span class="math"&gt;\(log(days)\)&lt;/span&gt;, because I read that doing so lets 
you combine data with different exposure durations, with the &lt;span class="math"&gt;\(log(days)\)&lt;/span&gt;'s 
coefficient indicating the change in rate over time. As it is, I feel like 
it would be difficult to find a statistical test that &lt;em&gt;did&lt;/em&gt; show some sort 
of relationship between counts and duration of exposure. So my model (in R 
code) was essentially (names changed for clarity): &lt;/p&gt;
&lt;p&gt;&lt;code&gt;glm(fibers ~ log(days 
alive), family = poisson(), data = 
treatment crabs)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which yielded:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;param&lt;/th&gt;
&lt;th&gt;Estimate&lt;/th&gt;
&lt;th&gt;Std. Error&lt;/th&gt;
&lt;th&gt;z value&lt;/th&gt;
&lt;th&gt;Pr(&amp;gt;|z|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;(Intercept)&lt;/td&gt;
&lt;td&gt;0.87576&lt;/td&gt;
&lt;td&gt;0.69172&lt;/td&gt;
&lt;td&gt;1.266&lt;/td&gt;
&lt;td&gt;0.205&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;log(num_of_days_alive)&lt;/td&gt;
&lt;td&gt;-0.07686&lt;/td&gt;
&lt;td&gt;0.18669&lt;/td&gt;
&lt;td&gt;-0.412&lt;/td&gt;
&lt;td&gt;0.681&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Which I interpret as "no, there isn't much of an association between counts and the different durations over which the counts accumulated", because of the small Estimate, relatively large Std. Error, and small Pr() for log(num_of_days_alive)."&lt;/p&gt;
&lt;p&gt;I kinda wish they'd checked the number of fibers suspended in the water of 
the jars somehow.&lt;/p&gt;
&lt;p&gt;I also wonder if sand crabs can vomit. This paper on decapod crustacean 
welfare in experiments [&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;] references a study on red rock crab digestive 
processes [&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;] that observed regurgitation/vomiting. So maybe?&lt;/p&gt;
&lt;p&gt;Back to the statistics, the authors report a relationship between fibers 
internalized and a 
decrease 
in days alive. I'm skeptical that there's anything to it. If they 
incorporated some sort of accounting for censored observations (maybe &lt;code&gt;lme4&lt;/code&gt; 
handles things somehow?) they don't mention it in the publication.&lt;/p&gt;
&lt;h1&gt;Wait, what happened to Bayes?&lt;/h1&gt;
&lt;p&gt;Oh.&lt;/p&gt;
&lt;h2&gt;Exponentially Distributed Survival&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/swpease/sand-crabs/blob/master/sand_crabs.ipynb"&gt;Jupyter Notebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Well, I tried fitting an Exponential distribution to the mortality data, per 
the suggestion found in this lecture from &lt;a href="https://youtu.be/Zi6N3GLUJmw?si=AowaUikcftuAtCCk&amp;amp;t=4882"&gt;Statistical Rethinking&lt;/a&gt;. I 
think it looks okay?&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 3" src="https://swpease.github.io/images/science/sand_crabs/exponential_72d.png"&gt;&lt;/p&gt;
&lt;p&gt;and to see where these curves go:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 4" src="https://swpease.github.io/images/science/sand_crabs/exponential_730d.png"&gt;&lt;/p&gt;
&lt;p&gt;My biggest concern is: what would the data have wound up looking like if the 
experiment went on for longer?&lt;/p&gt;
&lt;p&gt;The graphs get a bit muddled where the treatment and control fits overlap, 
but there's nothing fascinating going on at the overlaps, so I just left it 
as is rather than double the number of graphs.&lt;/p&gt;
&lt;h3&gt;Picking a prior&lt;/h3&gt;
&lt;p&gt;The exponential model only has one parameter, &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;. The mean of an 
exponential distribution is &lt;span class="math"&gt;\(1 / 
\lambda\)&lt;/span&gt;, so what's the expected mean?&lt;/p&gt;
&lt;p&gt;As far as expected survival times, &lt;a href="https://en.
wikipedia.org/wiki/Emerita_analoga"&gt;according to Wikipedia&lt;/a&gt;, most sand crabs 
die in the autumn of their second year. These crabs were collected on August 4,
so I guess I'll assume ~400 days survival time for crabs in their first year,
maybe just ~50 for crabs in their second year,
and I guess half in their first v second year,
so ~200+ days average survival time. Can you age crabs? I don't suppose 
they've got otoliths.&lt;/p&gt;
&lt;p&gt;With that in mind, and per the suggestion of the &lt;a href="https://youtu.be/Zi6N3GLUJmw?si=apBRHdMb0lCodpNI&amp;amp;t=5366"&gt;al ighty cElreath&lt;/a&gt; to set the priors on the log 
of the inverse of &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, so that they are easier to define, I went with 
a prior on said log inverse of &lt;span class="math"&gt;\(log(1/\lambda) \sim N(5, 1)\)&lt;/span&gt;, because &lt;span class="math"&gt;\(e^5 = 
148\)&lt;/span&gt; (and &lt;span class="math"&gt;\(e^4 = 55\)&lt;/span&gt; and &lt;span class="math"&gt;\(e^6 = 400\)&lt;/span&gt;). &lt;a href="https://github.com/swpease/sand-crabs/blob/master/simulated_exp_data.Rmd"&gt;R Markdown Notebook&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Expected Survival Times&lt;/h3&gt;
&lt;p&gt;So speaking of expected survival time, what does the Bayesian approach say 
it might be?&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 5" src="https://swpease.github.io/images/science/sand_crabs/forest_ridgeplot_90hdi.png"&gt;&lt;/p&gt;
&lt;p&gt;The posterior means of the mean survival time are 195 days for the control 
and 98 days for the treatment, but 
obviously 
it's pretty uncertain about the control, which makes sense given how few of 
the crabs died. You can see the uncertainty in the previous plot as well: those 
orange curves are pretty spread out. It's at least mildly encouraging that 
the control's posterior mean of the mean survival is fairly 
close to what I presumed 
based on the Wikipedia article. And the treatment mean survival time seems 
reasonable as well.&lt;/p&gt;
&lt;p&gt;And as for the contrast of &lt;span class="math"&gt;\(control - treatment\)&lt;/span&gt; expected survival times,
most of the 
probability 
mass is above zero, with a mean of 98 days (no, I didn't mistake it for the 
treatment's mean):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 6" src="https://swpease.github.io/images/science/sand_crabs/forest_90hdi_contrast.png"&gt;&lt;/p&gt;
&lt;p&gt;So the treatment crabs probably don't live as long as the control crabs on 
average, but it's pretty uncertain what exactly the difference is.&lt;/p&gt;
&lt;h3&gt;Rejected Prior&lt;/h3&gt;
&lt;p&gt;I also toyed with the idea of describing the survival as a mix of two 
populations, first year and second year crabs, by using a mix of two 
Exponential distributions. I decided it was 
just more 
complicated without providing any apparent benefits. The curves it was 
producing looked pretty similar to an Exponential distribution. &lt;a href="https://github.com/swpease/sand-crabs/blob/master/simulated_exp_data_spectrum.Rmd"&gt;R Markdown 
Notebook&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Sensitivity to the Prior&lt;/h3&gt;
&lt;p&gt;There's not a ton of data, so how much does the prior matter? Not too much. 
It pushes the averages around, but the relationships are pretty consistent. &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;log(mu)-prior&lt;/th&gt;
&lt;th style="text-align: center;"&gt;mu-prior&lt;/th&gt;
&lt;th style="text-align: center;"&gt;sigma-prior&lt;/th&gt;
&lt;th style="text-align: center;"&gt;mu-post-ctrl&lt;/th&gt;
&lt;th style="text-align: center;"&gt;mu-post-trt&lt;/th&gt;
&lt;th style="text-align: center;"&gt;mu-post-contrast&lt;/th&gt;
&lt;th style="text-align: center;"&gt;P(contrast &amp;gt; 0)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;4&lt;/td&gt;
&lt;td style="text-align: center;"&gt;55&lt;/td&gt;
&lt;td style="text-align: center;"&gt;1&lt;/td&gt;
&lt;td style="text-align: center;"&gt;180&lt;/td&gt;
&lt;td style="text-align: center;"&gt;92&lt;/td&gt;
&lt;td style="text-align: center;"&gt;87&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;5&lt;/td&gt;
&lt;td style="text-align: center;"&gt;148&lt;/td&gt;
&lt;td style="text-align: center;"&gt;1&lt;/td&gt;
&lt;td style="text-align: center;"&gt;194&lt;/td&gt;
&lt;td style="text-align: center;"&gt;98&lt;/td&gt;
&lt;td style="text-align: center;"&gt;96&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;6&lt;/td&gt;
&lt;td style="text-align: center;"&gt;400&lt;/td&gt;
&lt;td style="text-align: center;"&gt;1&lt;/td&gt;
&lt;td style="text-align: center;"&gt;215&lt;/td&gt;
&lt;td style="text-align: center;"&gt;104&lt;/td&gt;
&lt;td style="text-align: center;"&gt;111&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;4&lt;/td&gt;
&lt;td style="text-align: center;"&gt;55&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.5&lt;/td&gt;
&lt;td style="text-align: center;"&gt;140&lt;/td&gt;
&lt;td style="text-align: center;"&gt;85&lt;/td&gt;
&lt;td style="text-align: center;"&gt;55&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.94&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;5&lt;/td&gt;
&lt;td style="text-align: center;"&gt;148&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.5&lt;/td&gt;
&lt;td style="text-align: center;"&gt;182&lt;/td&gt;
&lt;td style="text-align: center;"&gt;104&lt;/td&gt;
&lt;td style="text-align: center;"&gt;79&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;6&lt;/td&gt;
&lt;td style="text-align: center;"&gt;400&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.5&lt;/td&gt;
&lt;td style="text-align: center;"&gt;251&lt;/td&gt;
&lt;td style="text-align: center;"&gt;131&lt;/td&gt;
&lt;td style="text-align: center;"&gt;119&lt;/td&gt;
&lt;td style="text-align: center;"&gt;0.95&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Gamma Distributed Survival&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/swpease/sand-crabs/blob/master/sand_crabs_gamma.ipynb"&gt;Jupyter Notebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The gamma distribution gives a sigmoidal survival curve. Maybe that'll look 
a bit better? I think it does, particularly at the start, though it also 
seems to have more posterior 
draws that aren't particularly believable. For instance,
look at the control fits' cutoffs on the right at over 700 days.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 7" src="https://swpease.github.io/images/science/sand_crabs/gamma_72d.png"&gt;
&lt;img alt="Figure 8" src="https://swpease.github.io/images/science/sand_crabs/gamma_730d.png"&gt;&lt;/p&gt;
&lt;h3&gt;Expected Survival Times&lt;/h3&gt;
&lt;p&gt;Overall, it's a similar story with the expected survival times here. The 
posterior 
means of the mean survival time are 195 days for the control 
and 84 days for the treatment, but 
once again 
it's pretty uncertain about the control. Actually, it's even more uncertain: 
the standard deviation in this case is 133, while it was merely 64 with the 
exponential model. This uncertainty propogates to the contrasts, though the 
probability density is still mostly above 0 (97% &amp;gt; 0), with a mean of 112.
&lt;img alt="Figure 9" src="https://swpease.github.io/images/science/sand_crabs/forest_ridgeplot_gamma_90hdi.png"&gt;
&lt;img alt="Figure 10" src="https://swpease.github.io/images/science/sand_crabs/contrast_gamma_90hdi.png"&gt;&lt;/p&gt;
&lt;h3&gt;Picking a Prior&lt;/h3&gt;
&lt;p&gt;This uses the same logic as above, but now needs two priors, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; 
(shape) and 
&lt;span class="math"&gt;\(\beta\)&lt;/span&gt; (rate), where for a gamma distribution &lt;span class="math"&gt;\(\mu = \alpha / \beta\)&lt;/span&gt; and 
&lt;span class="math"&gt;\(var = \alpha / \beta^2\)&lt;/span&gt;. I settled on &lt;span class="math"&gt;\(\alpha \sim Gamma(\alpha = 2, \beta = 
1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta \sim HalfNormal(\sigma = 0.02)\)&lt;/span&gt;, which gave mostly unabsurd 
survival curves in prior predictive simulation. &lt;/p&gt;
&lt;h3&gt;Rejected Prior&lt;/h3&gt;
&lt;p&gt;I also tried &lt;span class="math"&gt;\(\beta \sim LogNormal(\mu = -4, \sigma = 1)\)&lt;/span&gt;, but this did not 
seem to produce as good of fits to the data.&lt;/p&gt;
&lt;h3&gt;Sensitivity to the Prior&lt;/h3&gt;
&lt;p&gt;Changing the prior to &lt;span class="math"&gt;\(\beta \sim HalfNormal(\sigma = 0.2)\)&lt;/span&gt; yielded similar 
results. That's all I looked at because I'm sick of looking at survival curves.&lt;/p&gt;
&lt;h1&gt;Discussion / Conclusions&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Eating plastic is probably bad.&lt;/strong&gt; What a revelation. Really though, I am 
surprised at the posterior difference in expected survival time, because the 
experiment was designed to have environmentally relevant exposure levels. 
Do sand crabs have much shorter lifespans nowadays? Were the control crabs 
happy to have clean water (but not clean sediment!) for once? I wonder if it 
might be more accurate to think of the control as the effective "treatment", 
since the experimental treatment &lt;em&gt;ought&lt;/em&gt; to approximate the sand crabs' actual 
living conditions more closely.&lt;/p&gt;
&lt;p&gt;And can sand crabs 
vomit? The 
unanswered 
question. I suppose if they all had at least one fiber in them, they at the 
very least don't always vomit up accidentally ingested non-food. I wish 
they'd collected the poop.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stay tuned for part 2: egg stuff!&lt;/strong&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;"Anaesthesia of decapod crustaceans." de Souza Valente C. 2022.
https://doi.org/10.1016/j.vas.2022.100252&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;"Gastric processing and evacuation during emersion in the red rock 
crab, Cancer productus." Mcgaw IJ. 2007. https://doi.org/10.1080/10236240701393461&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Science"></category><category term="analysis"></category></entry><entry><title>Livescore Graphics Wishes</title><link href="https://swpease.github.io/livescore-graphics-wishes.html" rel="alternate"></link><published>2023-07-25T00:00:00-07:00</published><updated>2023-07-25T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2023-07-25:/livescore-graphics-wishes.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;WST has in the past provided livescores and recent results for matches. The livescores showed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Who was at the table and their current break.&lt;/li&gt;
&lt;li&gt;The current frame's score and points remaining.&lt;/li&gt;
&lt;li&gt;The match score.&lt;/li&gt;
&lt;li&gt;Average shot times for the match.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Strangely, they also offered the scores for each frame …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;WST has in the past provided livescores and recent results for matches. The livescores showed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Who was at the table and their current break.&lt;/li&gt;
&lt;li&gt;The current frame's score and points remaining.&lt;/li&gt;
&lt;li&gt;The match score.&lt;/li&gt;
&lt;li&gt;Average shot times for the match.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Strangely, they also offered the scores for each frame of the match, but only made it available after the match was finished, unless you went to &lt;a href="snooker.org"&gt;snooker.org&lt;/a&gt;, which somehow had these links mid-match.&lt;/p&gt;
&lt;p&gt;They are rolling out a new scoring site. A temporary one is currently up, which has had issues (e.g. wrong scores). I &lt;strong&gt;hope&lt;/strong&gt; that this new site takes more advantage of the shot-by-shot data, because I think that there is a lot of potential to make things more interesting.&lt;/p&gt;
&lt;h1&gt;My Wish List&lt;/h1&gt;
&lt;p&gt;The WST does not to my knowledge publish information about the frames of matches it oversees. Luckily, the &lt;a href="https://snookerscores.net/"&gt;WPBSA site&lt;/a&gt; has tournament histories with matches and frame information. This &lt;strong&gt;does not&lt;/strong&gt; include WST matches, but does include other snooker organizations, such as the Women's and Seniors' tours. In most cases, the frame data was manually recorded, so it is not there, but in some cases, such as the &lt;a href="https://snookerscores.net/tournament-manager/2023-british-womens-open/results"&gt;British Women's Open&lt;/a&gt;, some of the matches &lt;strong&gt;do&lt;/strong&gt; have what are called a &lt;strong&gt;"frame sheet"&lt;/strong&gt;. These frame sheets list the shot time and outcomes of the shots for each shot of the frame (&lt;a href="https://snookerscores.net/scoreboard/frame/9iwyicyxumm5vmp7iocqod7mwhnimjcg/sheet"&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Below are a couple of prototype graphics I've created from a WPBSA frame sheet from the 2023 British Women's Open final between Bai Yulu and Reanne Evans. Assuming that the WST has the data required to generate such a frame sheet for its own matches, I think things along these lines would be great additions to the livescoring site.&lt;/p&gt;
&lt;p&gt;Also, I'm aware that it should be "Bai" rather than "Yulu" in the legends. I just copied over the data.&lt;/p&gt;
&lt;h2&gt;1. Scores over Time&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Figure 1" src="https://swpease.github.io/images/snooker/livescores/frame_scores.png"&gt;&lt;/p&gt;
&lt;p&gt;This figure shows the scores of each player over the course of the frame. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The colored circles indicate which colored ball was potted at that time (hopefully obvious). &lt;/li&gt;
&lt;li&gt;The red "X"s indicate where a player fouled. In this match, Reanne fouled five times. &lt;/li&gt;
&lt;li&gt;The yellow band(s) indicate where in the match there were snookers required.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2. Shot Outcomes over Time&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Figure 2" src="https://swpease.github.io/images/snooker/livescores/frame_actions.png"&gt;&lt;/p&gt;
&lt;p&gt;This figure shows the outcome of each player's shots over the course of the frame.&lt;/p&gt;
&lt;p&gt;I like how this graph provides a snapshot of the match that is complementary to the first graph. You can see things such as: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Where a break went awry.&lt;/li&gt;
&lt;li&gt;Who played a good (dare I say, telling) safety resulting in fouls.&lt;/li&gt;
&lt;li&gt;Where a safety battle occurred.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Ending Remarks&lt;/h2&gt;
&lt;p&gt;These are just prototype graphs. I like them, but there are presumably other directions you can go with the data. I considered lumping the two together, but it looked so busy in my mental image that I didn't even actually try it.&lt;/p&gt;
&lt;p&gt;I do hope that the new WST livescore site takes better advantage of the frame data; you can really paint a picture with it.&lt;/p&gt;</content><category term="Snooker"></category><category term="analysis"></category></entry><entry><title>What Does Tournament Structure Matter?</title><link href="https://swpease.github.io/what-does-tournament-structure-matter.html" rel="alternate"></link><published>2023-05-31T00:00:00-07:00</published><updated>2023-05-31T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2023-05-31:/what-does-tournament-structure-matter.html</id><summary type="html">&lt;h1&gt;The Situation&lt;/h1&gt;
&lt;p&gt;"Oh boy, Jersey Shore!" you might be thinking. Sorry, not today.&lt;/p&gt;
&lt;p&gt;There are a few different tournament structures that they use in snooker. The three that I'm considering today are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Random draws:&lt;ul&gt;
&lt;li&gt;Tournaments: ShootOut, British Open&lt;/li&gt;
&lt;li&gt;Structure: What it says on the tin.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Flat draws:&lt;ul&gt;
&lt;li&gt;Tournaments: UK Championship …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;The Situation&lt;/h1&gt;
&lt;p&gt;"Oh boy, Jersey Shore!" you might be thinking. Sorry, not today.&lt;/p&gt;
&lt;p&gt;There are a few different tournament structures that they use in snooker. The three that I'm considering today are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Random draws:&lt;ul&gt;
&lt;li&gt;Tournaments: ShootOut, British Open&lt;/li&gt;
&lt;li&gt;Structure: What it says on the tin.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Flat draws:&lt;ul&gt;
&lt;li&gt;Tournaments: UK Championship prior to 2022&lt;/li&gt;
&lt;li&gt;Structure: Seed 1 v 128, 2 v 127, ..., 64 v 65&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tiered draws:&lt;ul&gt;
&lt;li&gt;Tournaments: World Championship, UK 2022&lt;/li&gt;
&lt;li&gt;Structure: Hard to describe. See &lt;a href="https://en.wikipedia.org/wiki/2023_World_Snooker_Championship"&gt;Wikipedia&lt;/a&gt; for an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In general, the tournaments try to strike a balance of maximizing the spotlight for the stars without being too harsh on the non-stars. People want to see Neil Robertson (for instance), so his qualifying matches are against a low-ranked player and are held over to the main event just in case (Astley 😠), or he gets a bye to the last 32.&lt;/p&gt;
&lt;p&gt;The common complaint with the flat draw structure is that it's unfair for the lower ranked players — particularly the new ones: "Congratulations on becoming a professional snooker player! Have fun playing Ronnie, Kyren, and Judd!" Poor Amine Amiri.&lt;/p&gt;
&lt;p&gt;The tiered draw structure prevents that scenario. However, it means that the lower ranked you are, the more matches you have to play. That aspect was what I was particularly interested in: who benefits, who doesn't? It can't magically be better for everyone. &lt;/p&gt;
&lt;p&gt;The top players come in at the Last 32. The common asterisks brought up seem to be "match sharpness" and the added risk of no ranking points for a first-match loss. But surely only a madman would say, "No thanks, I'll pass on the bye to the Last 32 so that I can be match sharp." or "Good thing I'm ranked #17 in time for the Worlds!".&lt;/p&gt;
&lt;p&gt;The bottom players have to play a whopping nine (9) rounds. On the one hand, they're playing lower ranked players compared to other structures, but on the other hand, nine rounds. Similarly, the two middle tiers of player have 8 and 7 rounds.&lt;/p&gt;
&lt;h2&gt;Before We Begin&lt;/h2&gt;
&lt;p&gt;There'a a lot of words coming up. You might want to just skip to the bottom four graphs. The results probably won't surprise you (at least, they didn't surprise me).&lt;/p&gt;
&lt;h2&gt;Data Source&lt;/h2&gt;
&lt;p&gt;As usual, the &lt;a href="https://api.snooker.org/"&gt;snooker.org API&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;The Tournaments&lt;/h1&gt;
&lt;p&gt;I'm looking at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;UK Championship 2015-2022&lt;ul&gt;
&lt;li&gt;Flat 2015-2021&lt;/li&gt;
&lt;li&gt;Tiered 2022&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;British Open 2021-2022&lt;ul&gt;
&lt;li&gt;Random&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ShootOut 2016-2022&lt;ul&gt;
&lt;li&gt;Random&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;World Championship 2019-2022&lt;ul&gt;
&lt;li&gt;Tiered&lt;/li&gt;
&lt;li&gt;Best of 11 pre-"Judgement Day" for 2019-2021&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lumping Tournaments Together&lt;/h2&gt;
&lt;p&gt;I'd prefer to lump together all tournamentes with a given structure together and take their averages. There's not many tournaments in each group, so just being able to go from 4 to 5 would be nice.&lt;/p&gt;
&lt;h3&gt;Flat Draws&lt;/h3&gt;
&lt;p&gt;The flat draw is easy: it's always the UK. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 1" src="https://swpease.github.io/images/snooker/tournament_structure/flat_tournaments.png"&gt;&lt;/p&gt;
&lt;p&gt;Overlapping dots are points that &lt;em&gt;were&lt;/em&gt; on the same spot, so I nudged them to make sure all were visible. If there were no players left at a given round, they're at zero (i.e. not on the chart).&lt;/p&gt;
&lt;h3&gt;Tiered Draws&lt;/h3&gt;
&lt;p&gt;The tiered draw tournaments' lump-ability largely depends on your belief in how much match length and tournament name matter. I'm convinced that match length is way overblown in terms of match outcomes, but am concerned about lumping the UK in with the Worlds. I'll plot the number of players who reached a given round, split out by their tier and the tournaments:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 2" src="https://swpease.github.io/images/snooker/tournament_structure/tiered_tournaments.png"&gt;&lt;/p&gt;
&lt;p&gt;The WC 2022 is indistinguishable from 2019-2021. I'm less confident with the UK, which looks a bit different, but that looks mostly due to the good performance of the 17-48 ranked players (which included Ding!).&lt;/p&gt;
&lt;h3&gt;Random Draws&lt;/h3&gt;
&lt;p&gt;The random draws are in the British Open and ShootOut, two very different tournaments. Here is the same graph as above, but for these tournaments:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 3" src="https://swpease.github.io/images/snooker/tournament_structure/random_tournaments.png"&gt;&lt;/p&gt;
&lt;p&gt;Sorry that it's getting a bit pointillist. In the British Opens, the 17-48 tier looks to have done a bit better at the expense of the 81+ tier, but... well it's not a huge difference. I'm not particularly interested in the random draw right now. Maybe I should just leave the British Open out. We'll cross that bridge when we come to it.&lt;/p&gt;
&lt;h3&gt;The Lumps&lt;/h3&gt;
&lt;p&gt;Actually, I'll just leave out the British Open and 2022 UK. I think including them would raise more doubts than the extra power is worth. I've looked at the subsequent charts in both scenarios, and they look close, so in the end it doesn't much matter.&lt;/p&gt;
&lt;h1&gt;Average Outcomes&lt;/h1&gt;
&lt;h2&gt;Preliminary&lt;/h2&gt;
&lt;h3&gt;Using Averages&lt;/h3&gt;
&lt;p&gt;Are the averages useful metrics here? i.e. are any of the player counts wildly different between tournaments? Looking at the above three graphs, there are a few cases where things are pretty variable, but in general the averages look useful.&lt;/p&gt;
&lt;h3&gt;How to Look at the Following Graphs&lt;/h3&gt;
&lt;p&gt;Because of the variable number of rounds that players play in the different tournament structures, there's not exactly an obvious way to compare them. You could...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read things left to right.&lt;ul&gt;
&lt;li&gt;This looks at the number of rounds that players have made it through.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Read things right to left.&lt;ul&gt;
&lt;li&gt;This looks at how deep into a tournament players are getting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Look at a middle point (Last 32)&lt;ul&gt;
&lt;li&gt;This is where all the tournament types sync up.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Graphs, Split up by Player Tiers&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Figure 4" src="https://swpease.github.io/images/snooker/tournament_structure/avg_1-16.png"&gt;
&lt;img alt="Figure 5" src="https://swpease.github.io/images/snooker/tournament_structure/avg_17-48.png"&gt;
&lt;img alt="Figure 6" src="https://swpease.github.io/images/snooker/tournament_structure/avg_49-80.png"&gt;
&lt;img alt="Figure 7" src="https://swpease.github.io/images/snooker/tournament_structure/avg_81plus.png"&gt;&lt;/p&gt;
&lt;p&gt;I've rounded the average counts to the nearest whole numbers, because the graphs look way better this way. One issue that can't really go away is the differing number of rounds and entry points across tournament structure and player seeding. The result of this is that the lines you naturally try to make by connecting the dots aren't going to be quite right in most cases (e.g. L128 and L64 has L80 and L48 shoved in between).&lt;/p&gt;
&lt;h2&gt;Assessing the Graphs&lt;/h2&gt;
&lt;h3&gt;Tier 1-16&lt;/h3&gt;
&lt;p&gt;Coming as no surprise, tiered tournaments are beneficial to these players. Unless they lose first round, which is now more likely.&lt;/p&gt;
&lt;h3&gt;Tier 17-48&lt;/h3&gt;
&lt;p&gt;This group looks like it's most heavily footing the bill in the tiered tournaments. I mean, just look at that graph — they do worse than in the flat draws.&lt;/p&gt;
&lt;h3&gt;Tier 49-80&lt;/h3&gt;
&lt;p&gt;Tiered tournaments seem to be beneficial to these players.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They tend to get about as deep into the tiered or flat draw tournaments.&lt;/li&gt;
&lt;li&gt;They are a bit more likely to get to the Last 32 in tiered tournaments.&lt;/li&gt;
&lt;li&gt;They are more likely to win at least one match (and two and three matches) in tiered tournaments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Tier 81+&lt;/h3&gt;
&lt;p&gt;It's more mixed for these players.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They tend to get farther into flat draw tournaments.&lt;/li&gt;
&lt;li&gt;They are less likely to get to the Last 32 in tiered tournaments.&lt;/li&gt;
&lt;li&gt;As with the tier 49-80, they are more likely to win at least one match (and two and three matches) in tiered tournaments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;My Conclusions&lt;/h1&gt;
&lt;p&gt;My assessment of the above graphs is about what I thought the case would be beforehand.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tiered tournaments are good for the top players, with extra risk of a first-match exit due to harder opponents.&lt;/li&gt;
&lt;li&gt;Tiered tournaments are bad for players just outside the top 16. Like the top players, they have harder matches from the start, but they have more rounds to get through.&lt;/li&gt;
&lt;li&gt;Tiered tournaments are arguably beneficial for low-to-mid-ranked players. They have easier early-rounds matches, but have to get through more rounds.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- So how do these three tournament structures' average outcomes look?

![Figure 3]({static}/images/snooker/tournament_structure/avg_outcomes.png)

Firstly, this image appears too small (at least on my screen). You can get the full-sized image by right clicking on it and clicking on "Open Image in New Tab". --&gt;</content><category term="Snooker"></category><category term="analysis"></category></entry><entry><title>It's A Coin Flip! (Part 2)</title><link href="https://swpease.github.io/its-a-coin-flip-part-2.html" rel="alternate"></link><published>2023-05-16T00:00:00-07:00</published><updated>2023-05-16T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2023-05-16:/its-a-coin-flip-part-2.html</id><summary type="html">&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;Continuing my attempts to demonstrate that winning shorter matches in snooker should not be belittled, here I wanted to look at the Championship League. The impetus for this is that John Higgins recently won the tournament for the fourth time, beating Judd Trump (who was also going for his …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;Continuing my attempts to demonstrate that winning shorter matches in snooker should not be belittled, here I wanted to look at the Championship League. The impetus for this is that John Higgins recently won the tournament for the fourth time, beating Judd Trump (who was also going for his fourth win). That's 7 out of 17 wins between them. Clearly there's a knack for these things.&lt;/p&gt;
&lt;p&gt;The format of Championship League is kind of confusing, but here is a general outline:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two stages:&lt;ul&gt;
&lt;li&gt;Group Stage (7 total, 7 players each group)&lt;/li&gt;
&lt;li&gt;Winner's Group (1 total, the 7 Group Stage winners)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Best of 5's.&lt;/li&gt;
&lt;li&gt;Each group begins with a round robin format:&lt;ul&gt;
&lt;li&gt;If you're in the top four after the round robin, you enter a play-off.&lt;/li&gt;
&lt;li&gt;If you're #5, you're put into the next of the Group Stages.&lt;/li&gt;
&lt;li&gt;If you're in the bottom two, you're eliminated from the tournament.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Play-off has the standard knock-out format.&lt;ul&gt;
&lt;li&gt;If you win, you go on to the Winner's Group&lt;/li&gt;
&lt;li&gt;If you don't, you're put into the next of the Group Stages.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, to win the tournament, you need to win a Group Stage, and then win the Winner's Group. 4 of the 7 players in each group are put into the next Group Stage group. Except for after the last Group Stage group, of course. Group.&lt;/p&gt;
&lt;p&gt;This is invitational, so players get to pick which group stage they'd like to enter into the tournament at.&lt;/p&gt;
&lt;h1&gt;Set-up&lt;/h1&gt;
&lt;p&gt;I decided to look at it from the skeptical perspective: &lt;strong&gt;Winning the tournament is random chance&lt;/strong&gt;. This means that I'm giving everyone a 1 in 7 chance of winning a given group. &lt;/p&gt;
&lt;p&gt;My thinking was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;John Higgins is one of the best players.&lt;/li&gt;
&lt;li&gt;The odds of his four wins in a "roll of the dice" scenario is presumably quite low.&lt;/li&gt;
&lt;li&gt;The lower the math works out for the odds to be, the stronger my position that these wins are skill-based.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;The Math&lt;/h1&gt;
&lt;h2&gt;Winning One Tournament&lt;/h2&gt;
&lt;h3&gt;Probabilities, Visualized&lt;/h3&gt;
&lt;p&gt;Pictures are nice. To see what I'm doing, here is an example probability tree, for someone entering in Group 5 (and ignoring the Winner' Group):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Probability Tree" src="https://swpease.github.io/images/snooker/championship_league/snooker_CL_p_tree.png"&gt;&lt;/p&gt;
&lt;p&gt;As an example of how to look at this, imagine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The player is one of the 4 players who don't win Group 5, but aren't eliminated (i.e. not in the bottom 2 of the round robin phase). &lt;/li&gt;
&lt;li&gt;Then, they win Group 6.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The probability of getting to the Winner's Group by that route is (4/7)*(1/7) = 4/49. &lt;/p&gt;
&lt;p&gt;Add up all of the routes-to-the-Winner's-Group probabilities to get the total probability of reaching the Winner's Group.&lt;/p&gt;
&lt;h3&gt;Mathematical Formulation&lt;/h3&gt;
&lt;p&gt;The probability of advancing to the Winner's Group is:
&lt;/p&gt;
&lt;div class="math"&gt;$$  \sum_{r=0}^{n - 1} p_{win}*p_{next}^{r} $$&lt;/div&gt;
&lt;p&gt;
Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(n\)&lt;/span&gt; is the initial number of rounds remaining.&lt;ul&gt;
&lt;li&gt;For example, if you entered in round 5 (always of 7), n is 3 (rounds 5, 6, and 7).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_{win}\)&lt;/span&gt; is the probability of winning a group.&lt;ul&gt;
&lt;li&gt;Here, this means 1/7.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(p_{next}\)&lt;/span&gt; is the probability of going to the next Group Stage group.&lt;ul&gt;
&lt;li&gt;Here, this means 4/7.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since I'm assuming the probability of winning a group is always the same, the probability of winning the tournament is just 1/7 times the probability of reaching the Winner's Group:
&lt;/p&gt;
&lt;div class="math"&gt;$$  p_{win} * \sum_{r=0}^{n - 1} p_{win}*p_{next}^{r} $$&lt;/div&gt;
&lt;h3&gt;In Python&lt;/h3&gt;
&lt;p&gt;Here is the above equation, in Python!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;p_win_group_stage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_rounds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_win&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_next&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    num_rounds: # of group stage rounds the player has, i.e # of opportunities to reach the final stage.&lt;/span&gt;
&lt;span class="sd"&gt;    p_win: Probability of winning a round (group or final)&lt;/span&gt;
&lt;span class="sd"&gt;    p_next: Probability of going to next Group Stage group (non-final rounds of group stage)&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;p_win&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p_next&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_rounds&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;p_win_tourn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_rounds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_win&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_next&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    num_rounds: # of group stage rounds the player has, i.e # of opportunities to reach the final stage.&lt;/span&gt;
&lt;span class="sd"&gt;    p_win: Probability of winning a round (group or final)&lt;/span&gt;
&lt;span class="sd"&gt;    p_next: Probability of going to next Group Stage group (non-final rounds of group stage)&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p_win&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;p_win_group_stage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_rounds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_win&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Winning Multiple Tournaments&lt;/h2&gt;
&lt;p&gt;We have a binary outcome for each tournament (Win/Loss). Ordinarily, we could model our #-of-Wins with a binomial distribution. However, in our case, that doesn't work, because players can enter in different rounds of the Group Stage. If they do, then they have different probabilities of winning each tournament. This calls for the... &lt;strong&gt;Poisson Binomial distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;The Poisson Binomial Distribution&lt;/h3&gt;
&lt;p&gt;The idea is just like the binomial distribution, except that each of the independent Bernoulli trials can have its own probability of success.&lt;/p&gt;
&lt;p&gt;Both the Poisson Binomial and binomial distributions are the sums of their Bernoulli trials. As such, both can be found by taking the &lt;a href="https://en.wikipedia.org/wiki/Convolution_of_probability_distributions"&gt;convolution&lt;/a&gt; of the Bernoulli trials.&lt;/p&gt;
&lt;p&gt;This is all well and good, but it's not like you're going to calculate it by hand or write the function to calculate it. Well, I did out of curiosity, but then I just found a Python package that does it for me.&lt;/p&gt;
&lt;h1&gt;The Odds of Higgins's Performance&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;1 in 900&lt;/strong&gt; (ish, in a roll-of-the-dice scenario)&lt;/p&gt;
&lt;p&gt;John Higgins played in 14 of the 17 Invitational Championship Leagues, entering in the following rounds (most recent first):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# W  W           W  W&lt;/span&gt;
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So he won 3 events where he entered in the final Group Stage group!&lt;/p&gt;
&lt;h2&gt;The Poisson Binomial Code&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;poibin&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PoiBin&lt;/span&gt; &lt;span class="c1"&gt;# An un-published package I found on GitHub. There are other options.&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;p_win&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;
    &lt;span class="n"&gt;p_next&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;
    &lt;span class="n"&gt;max_rounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;
                      &lt;span class="c1"&gt;# W  W           W  W&lt;/span&gt;
    &lt;span class="n"&gt;entered_in_round&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;num_rounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;max_rounds&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;entered_in_round&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;tourn_win_probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_win_tourn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_rds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_win&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p_next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num_rds&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;num_rounds&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;pb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PoiBin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tourn_win_probs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p_over_3_wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;pb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;p_over_3_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Alternate Odds&lt;/h2&gt;
&lt;p&gt;I also checked the odds if we assume that Higgins will definitely get to the group playoffs (i.e. past the round robin phase), but the playoffs are still a dice roll. In code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;    &lt;span class="n"&gt;p_win&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
    &lt;span class="n"&gt;p_next&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this scenario, the odds of Higgins's performance are way way better: &lt;strong&gt;1 in 7&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;Just adding in a little bit of an edge in favor of Higgins quickly improves the odds of his performance, down to believable levels. Here's a full table of odds against Higgins's performance at various probabilities of winning (p_win) and getting to the next Group Stage group (p_next), with blue highlights on the (rounded) two scenarios I've considered above:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Odds Table" src="https://swpease.github.io/images/snooker/championship_league/higgins_odds_table.png"&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Even just rounding 1/7 and 4/7 to two digits (0.14 and 0.57) yields pretty different odds. It doesn't take much of a shift away from roll-of-the-dice to wind up in believeable odds territory, so my argument isn't air-tight, but it's something!&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Code for the Above Table&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;round_win_ps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;round_x_ps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;48&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;max_rounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;
                  &lt;span class="c1"&gt;# W  W           W  W&lt;/span&gt;
&lt;span class="n"&gt;entered_in_round&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;num_rounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;max_rounds&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;rd&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;entered_in_round&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;odds_over_3_wins_all&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;win_p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;round_win_ps&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;odds_over_3_wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x_p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;round_x_ps&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;p_wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_win_tourn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_rds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;win_p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num_rds&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;num_rounds&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;pb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PoiBin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;pb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;odds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="n"&gt;odds_over_3_wins&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;odds&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;odds_over_3_wins_all&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;odds_over_3_wins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1&gt;Bonus: Brute Force!&lt;/h1&gt;
&lt;p&gt;I'm always a bit worried about my math when dealing with probabilities — it's so easy to make a mistake! So, I decided to make a brute force alternate formulation to check my math, and I guess kind of check my code. Voila:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;brute_force_single_tourn_outcome&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_rounds&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;rn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;rn&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  &lt;span class="c1"&gt;# Won&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;rn&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  &lt;span class="c1"&gt;# Eliminated&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num_rounds&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# Eliminated b/c no more rounds.&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# Through to next round of group stage.&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;brute_force_single_tourn_outcome&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_rounds&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;brute_force_p_win_tourn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100_000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;brute_force_single_tourn_outcome&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Snooker"></category><category term="analysis"></category></entry><entry><title>It's A Coin Flip!</title><link href="https://swpease.github.io/its-a-coin-flip.html" rel="alternate"></link><published>2022-05-25T00:00:00-07:00</published><updated>2022-05-25T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2022-05-25:/its-a-coin-flip.html</id><summary type="html">&lt;h1&gt;The Problem&lt;/h1&gt;
&lt;p&gt;While I have only been watching snooker since the 2019 World Championship, I gather than matches have become shorter over time. People tend to complain that the shorter formats add a larger element of luck to the outcomes. I decided to try and put some numbers to this …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;The Problem&lt;/h1&gt;
&lt;p&gt;While I have only been watching snooker since the 2019 World Championship, I gather than matches have become shorter over time. People tend to complain that the shorter formats add a larger element of luck to the outcomes. I decided to try and put some numbers to this perception.&lt;/p&gt;
&lt;h1&gt;The Data&lt;/h1&gt;
&lt;p&gt;I got the data from &lt;a href="snooker.org"&gt;snooker.org&lt;/a&gt;'s API. I took data from the 2015/16 season through to the end of the past season (2021/22). I picked this timespan because it looks like the current rankings system &lt;a href="http://api.snooker.org/help.html#RankingTypes"&gt;started then&lt;/a&gt;. I looked at ranking events only, and only those first rounds of 128 players. I restricted to the round of 128 to try and keep things as even as possible (full draw, no "hot" players, etc.). I omitted walkovers, of which there are often one or two, and sometimes a lot (Gibralter Open 2022). To summarize:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Seasons 2015/16 – 2021/22&lt;/li&gt;
&lt;li&gt;Ranking events&lt;/li&gt;
&lt;li&gt;First round only, of 128 players&lt;/li&gt;
&lt;li&gt;Walkovers excluded&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Basic Comparison&lt;/h1&gt;
&lt;h2&gt;Upsets by Tournament&lt;/h2&gt;
&lt;p&gt;Let's start by looking at things on a tournament-by-tournament basis over time, because that's how the data comes at us. What percentage of matches per tournament were upsets?&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table 1" src="https://swpease.github.io/images/snooker/snooker_pct_upsets_by_season.png"&gt;&lt;/p&gt;
&lt;p&gt;Hm. This isn't &lt;em&gt;super&lt;/em&gt; informative... Let's ignore the seasons, group the tournaments by match length, and split out Top 16 seeds from the others:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table 2" src="https://swpease.github.io/images/snooker/snooker_num_upsets_by_tourn.png"&gt;&lt;/p&gt;
&lt;p&gt;I guess longer matches make upsets a bit rarer, but it's hardly night and day. A handy thing about this plot is the two y-axes (# Upsets) go to 100%, so we can use the heights of the dots between the Top 16 seeds and the non-Top 16 seeds for percent-wise comparison. &lt;/p&gt;
&lt;p&gt;What are the actual numbers? &lt;/p&gt;
&lt;h2&gt;Upset Percentages&lt;/h2&gt;
&lt;p&gt;Now let's look at the upset percentages of all tournaments combined, split out by match length. I'll also split out the Top 16 seeds&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; from the sub-Top 16 higher seeds. (See below&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; for counts of the matches in each sub-group, so we know how many we're talking about. Important, but busy.) I'll include the Shoot-Out and British Open here and in any subsequent tables, even though I don't pay them much attention, because an extra line on a table is easier to ignore than extra lines/dots on a chart. Let's see:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table 4" src="https://swpease.github.io/images/snooker/snooker_total_upset_pct.png"&gt;&lt;/p&gt;
&lt;p&gt;So, looking at Best of 9's – 11's, upsets are less likely in longer matches. &lt;strong&gt;We can expect one more Top 16 Seed to be upset in Best of 7's compared to Best of 11's (2 -&amp;gt; 3, of 16), and three more non-Top 16 Seeds (13 -&amp;gt; 16, of 48)&lt;/strong&gt;. For reference: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;20% = 1 in 5 &lt;/li&gt;
&lt;li&gt;17% = 1 in 6 &lt;/li&gt;
&lt;li&gt;14% = 1 in 7 &lt;/li&gt;
&lt;li&gt;12% = 1 in 8 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Case closed! Right? Well... these matches aren't all between the same seedings each time. How much does that change things?&lt;/p&gt;
&lt;h1&gt;Factoring in Match Difficulty... or "Math with Opinions"&lt;/h1&gt;
&lt;h2&gt;Upset Potentials Defined&lt;/h2&gt;
&lt;p&gt;Seed 2 losing to seed 42 is a bigger upset than seed 42 losing to seed 82. To account for this aspect of the rankings, I converted the seeds using the base-2 logarithm, which I'll write as &lt;code&gt;log2(seed)&lt;/code&gt;. If you're unfamiliar with logarithms, here are some reference values to get the gist:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table 6" src="https://swpease.github.io/images/snooker/snooker_lg_ref.png"&gt;&lt;/p&gt;
&lt;p&gt;I created a metric from these logarithmed seedings that I called &lt;strong&gt;upset potential&lt;/strong&gt;, which is just the size of the difference in the match's two players' logarithmed seedings, i.e.:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;upset potential&lt;/strong&gt; = &lt;code&gt;log2(higher seed) - log2(lower seed)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I set any amateurs' seeding as 128. I also think of the upset potential as equating to &lt;strong&gt;skill gap&lt;/strong&gt;. For instance, Yan (16) losing to Ding (32) has an upset potential of 1.00, and Ding losing to Ursenbacher (62) has an upset potential of 0.95. Does that sound about right to you?&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2&gt;Splitting Out the Upset Potentials&lt;/h2&gt;
&lt;p&gt;Here are the average upset potentials, split out by match length:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table 7" src="https://swpease.github.io/images/snooker/snooker_upset_potential.png"&gt;&lt;/p&gt;
&lt;p&gt;A smaller upset potential means the match had players closer in seeding. So we see that, moving from Best of 7 to Best of 11, the average upset potential increased, which means that &lt;strong&gt;the higher seeded players had easier matches on average with increasing match length&lt;/strong&gt;. How much easier? Well, the Top 16 Seeds had on average an upset potential in their matches 0.4 greater in Best of 11 compared to Best of 9. That's like playing Seed 28 instead of Seed 21, Seed 42 instead of Seed 32, or Seed 84 instead of Seed 64!&lt;/p&gt;
&lt;p&gt;And the same info, further split by whether an upset occurred:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table 8" src="https://swpease.github.io/images/snooker/snooker_upset_potential_split.png"&gt;&lt;/p&gt;
&lt;p&gt;So, for a given match length, the upsets have a smaller average upset potential compared to the non-upsets, i.e. upsets happen when the players are closer in skill level. Obviously. This happens across the board. Wait... what about the Top 16 seeds in the Best of 11? Top 16 seeds have been more likely to lose against &lt;em&gt;worse&lt;/em&gt; players in Best of 11s these past several years! Oh dear.&lt;/p&gt;
&lt;h2&gt;Splitting Out the Upset Potentials Even More&lt;/h2&gt;
&lt;h3&gt;Upset Percents, Grouped by Upset Potential&lt;/h3&gt;
&lt;p&gt;Well, so how does a higher seed do for a given upset potential, i.e. against players of the same skill gap, across the match lengths? Here is the data, rounding down the upset potentials to the nearest whole number:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table 9" src="https://swpease.github.io/images/snooker/snooker_u_p_barplt_log2.png"&gt;&lt;/p&gt;
&lt;p&gt;50% upsets is the proverbial coin flip. The blue / orange / green bars show what has &lt;em&gt;actually happened&lt;/em&gt;, while the black lines at the top of each bar are the 95% confidence intervals. Essentially, the longer the black line, the less trust you should put in the corresponding colored bar staying where it is as more data is collected. Some of these are pretty huge (Top 16 upset potential of 0, 1, 6, 7). I'm guessing there aren't many data points... Nope. See here&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Well, in that case, let's use that other upset potential metric from Footnote &lt;sup id="fnref2:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; and see what happens. The counts look better&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt;, so how does the plot look?:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table 10" src="https://swpease.github.io/images/snooker/snooker_u_p_barplt_ceillog2div4.png"&gt;&lt;/p&gt;
&lt;p&gt;So it looks pretty similar, except at higher upset potentials for Top 16 matches, which was exactly the point.&lt;/p&gt;
&lt;h4&gt;Conclusions&lt;/h4&gt;
&lt;p&gt;What can we get out of these plots? &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;At a given upset potential (skill gap), there are more instances of the expected pattern of increasing match length yielding decreasing upset % &lt;em&gt;not&lt;/em&gt; happening than happening.&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;At a given skill gap, the differences in upset % usually and at most equate to differences of a few more/fewer upsets.&lt;/strong&gt; Maximums: &lt;ul&gt;
&lt;li&gt;For Top-16 Seeds: 3 fewer in Best of 11 vs Best of 7 at upset potential of 2.&lt;/li&gt;
&lt;li&gt;For non-Top-16 Seeds: 4 fewer in Best of 11 vs Best of 9 (note: not 7!) at upset potential of 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;At a given match length, increasing skill gap makes upsets less and less likely, with the notable exception of Top 16 seeds in Best of 11's, for which the opposite trend is true.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Yet Another View&lt;/h2&gt;
&lt;h3&gt;Percent Realized Upset Potential&lt;/h3&gt;
&lt;p&gt;One last way I wanted to look at the data is comparing the total upset potential for a tournament against the amount of that upset potential that actually occurred.&lt;/p&gt;
&lt;p&gt;"Percent realized upset potential" a lot of words; let's unpack it. We've established what "upset potential" means. "Realized" refers to the upsets that actually happened. "Percent" refers to totaling the realized upset potentials, and dividing by the total upset potentials over all the matches in the tournament round.&lt;/p&gt;
&lt;p&gt;A simple example: only two matches in the round, Match 1 has an upset potential of 3, while Match 2 has an upset potential of 1. If neither is an upset, the percent realized upset potential is 0%. If both are upsets, it is 100%. If Match 1 is an upset while Match 2 isn't, it is 3/4 = 75%. If Match 2 is an upset while Match 1 isn't, it is 1/4 = 25%.&lt;/p&gt;
&lt;p&gt;Here's what it looks like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table 11" src="https://swpease.github.io/images/snooker/snooker_pct_realized_u_p.png"&gt;&lt;/p&gt;
&lt;p&gt;There seem to be a few aspects in favor of either side of the argument.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Points in favor of match length mattering:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Shoot-Out and British Open are at the upper end of percent realized upset potential.&lt;/li&gt;
&lt;li&gt;The Best of 7's at similar total upset potential to longer matches are also at the upper end of percent realized upset potential in that subset.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, in both cases there are not very many data points. Even so, I suspect neither the Shoot-Out nor the British Open data will budge much in coming years, given their unique formats.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Points in favor of match length not mattering:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Best of 9's seem to have a similar distribution to the Best of 7's at the lower total upset potentials, along with a similar distribution to the Best of 11's at the higher total upset potentials. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, a general point about the graph:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The percent realized upset potential seems to get both less variable and on average smaller as the total upset potential increases. However, the match lengths tend to increase as total upset potential increases, confounding the relationship. &lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;I think the major takeaways from this analysis are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Upsets are more likely in shorter matches.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shorter matches tend to be against harder opponents for higher seeds.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;At a given skill gap, there isn't a consistent relationship between match length and the chances of an upset.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's all for now! I hope you at least gave the graphs a good look.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Note I said "Seeds", not "Ranking". So the defending champion is Seed 1. Yes, it's a smidge imprecise, but using rankings would have been a lot more effort for minimal gain.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;img alt="Table 5" src="https://swpease.github.io/images/snooker/snooker_split_upset_pct.png"&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;I know it's not perfect, but I tried a more complicated transformation, namely &lt;code&gt;1 + log2(ceil(seeding / 4))&lt;/code&gt;, which seemed closer to correct to me, but the graphs wound up looking basically the same, so I decided to keep the simpler transformation.&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;&lt;img alt="Table 6 supp_1" src="https://swpease.github.io/images/snooker/snooker_u_p_bin_counts_log2.png"&gt;
 &lt;img alt="Table 6 supp_2" src="https://swpease.github.io/images/snooker/snooker_u_p_bin_counts_log2_non-top16_plt.png"&gt;
&lt;img alt="Table 6 supp_3" src="https://swpease.github.io/images/snooker/snooker_u_p_bin_counts_log2_top16_plt.png"&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;&lt;img alt="Table 7 supp" src="https://swpease.github.io/images/snooker/snooker_u_p_bin_counts_stuffdiv4.png"&gt;
&lt;img alt="Table 7 supp_2" src="https://swpease.github.io/images/snooker/snooker_u_p_bin_counts_stuffdiv4_non-top16_plt.png"&gt;
&lt;img alt="Table 7 supp_3" src="https://swpease.github.io/images/snooker/snooker_u_p_bin_counts_stuffdiv4_top16_plt.png"&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Snooker"></category><category term="analysis"></category></entry><entry><title>Shaving</title><link href="https://swpease.github.io/shaving.html" rel="alternate"></link><published>2019-04-24T00:00:00-07:00</published><updated>2019-04-24T00:00:00-07:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2019-04-24:/shaving.html</id><summary type="html">&lt;h1&gt;The Options&lt;/h1&gt;
&lt;p&gt;I have had to shave regularly, since some time in high school. I have fairly thick hair, particularly in the chin and mustache region. It can be a hassle. I've tried a few options over the years:
1. Buying cartridges in stores.
2. Subscribing to online clubs.
3 …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;The Options&lt;/h1&gt;
&lt;p&gt;I have had to shave regularly, since some time in high school. I have fairly thick hair, particularly in the chin and mustache region. It can be a hassle. I've tried a few options over the years:
1. Buying cartridges in stores.
2. Subscribing to online clubs.
3. Safety razors.&lt;/p&gt;
&lt;p&gt;They each have their pros and cons, which I'll go over.&lt;/p&gt;
&lt;h3&gt;Buying Cartridges in Stores&lt;/h3&gt;
&lt;p&gt;The classic for anyone who isn't geriatric. I remember when my dad taught me to shave. It was basically, "spray some shaving cream into your hand, rub it over your face, then rub this razor over your face..." There's really nothing to it.&lt;/p&gt;
&lt;p&gt;I like shaving with these the best. I have always been a fan on the two-bladed cartridges. Three is okay, but I dislike how bulky the larger-count cartridges are. I also feel like they're a complete waste, given how effective two blades have been for me.&lt;/p&gt;
&lt;p&gt;The only issue for me is cost. I recently checked out the cartridges selection at Fred Meyer, and saw my trusty Sensor Excels at 10 for $30. That's just too much. This price is what drove me to try option #2 on the list.&lt;/p&gt;
&lt;h3&gt;Subscribing to Online Clubs&lt;/h3&gt;
&lt;p&gt;I tried out Dollar Shave Club. I went with the two-bladed model. At first I was pleased with it: it did the job well and cost a fraction of the price (I think ~ $1 / cartridge).&lt;/p&gt;
&lt;p&gt;But then, I visited my parents, where I still have some of the classic cartridges (both Sensor Excel and Mach 3). It felt like heaven to my face in comparison.&lt;/p&gt;
&lt;p&gt;If I had to guess, I'd say that the issue was the flimsy construction, which might cause some shaking / other motion that leads to uneven speeds across my face, leading to more tugging. But that's just conjecture.&lt;/p&gt;
&lt;p&gt;Which led me to option #3 on the list.&lt;/p&gt;
&lt;h3&gt;Safety Razors&lt;/h3&gt;
&lt;p&gt;I performed a little research using some relevant subreddit. I forget the name. But I wound up going with Maggard for my razor, styptic, alum block, brush, a variety pack of blades, and 'sapone di barba' or whatever.&lt;/p&gt;
&lt;p&gt;I've been using it for close to a year now, and my conclusion is: it's okay. I haven't been this nonplussed by a hyped alternative product since organic peanut butter.&lt;/p&gt;
&lt;p&gt;I did learn why dads teaching their sons how to shave is actually a thing: there's a lot more going on with safety razor shaving compared to cartridge shaving. You gotta lather the shaving cream, angle the razor, be careful about the grain...&lt;/p&gt;
&lt;p&gt;But, I still just prefer the ease and comfort of a Gillette cartridge and canned shaving cream. Sure, my &lt;em&gt;sapone&lt;/em&gt; smells nice, but it's inconvenient and expensive. It is more common for me to miss spots with my safety razor as well, stemming from the fact that it's not as comfortable on my face. Plus I'm still trying to find my optimum razor blades, an area for which reviews aren't really much use, since other reviewers don't have your face.&lt;/p&gt;
&lt;p&gt;That being said, the cartridge is not 10+ times the price better. So I'll stick with my safety razor.&lt;/p&gt;</content><category term="Misc"></category><category term="grooming"></category></entry><entry><title>Setting up Gunicorn with systemd on Ubuntu</title><link href="https://swpease.github.io/setting-up-gunicorn-with-systemd-on-ubuntu.html" rel="alternate"></link><published>2019-01-25T00:00:00-08:00</published><updated>2019-01-25T00:00:00-08:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2019-01-25:/setting-up-gunicorn-with-systemd-on-ubuntu.html</id><summary type="html">&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Gunicorn provides &lt;a href="http://docs.gunicorn.org/en/stable/deploy.html"&gt;a number of ways&lt;/a&gt; for you to handle running it in deployment. You need a supervisor, which is just a process that monitors and manages some other process. So you can tell it things like, 'start gunicorn when someone first requests a webpage' and 'reload gunicorn when …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Gunicorn provides &lt;a href="http://docs.gunicorn.org/en/stable/deploy.html"&gt;a number of ways&lt;/a&gt; for you to handle running it in deployment. You need a supervisor, which is just a process that monitors and manages some other process. So you can tell it things like, 'start gunicorn when someone first requests a webpage' and 'reload gunicorn when it crashes'.&lt;/p&gt;
&lt;p&gt;In skimming over the options that Gunicorn listed, I initially thought of using Supervisor, probably because I had seen it mentioned before. I am wary of my disposition to trusting things just because I've heard of them, but decided to look into it anyway. I saw that I would need to install Python 2 to use it, and put my foot down.
I decided to try out systemd because it was a built-in tool, and I am suspicious of taking on dependencies for what seem to be minimal tasks. It also seemed the most generalizable thing to learn.&lt;/p&gt;
&lt;h1&gt;Research&lt;/h1&gt;
&lt;p&gt;The best basic introduction I found was on &lt;a href="https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files#anatomy-of-a-unit-file"&gt;Digital Ocean&lt;/a&gt;.
The systemd creators have their documentation as well: &lt;a href="https://www.freedesktop.org/software/systemd/man/systemd.service.html#"&gt;systemd&lt;/a&gt;, &lt;a href="https://www.freedesktop.org/software/systemd/man/systemctl.html"&gt;systemctl&lt;/a&gt;.
More advanced topics, such as security and setting up your own software to handle systemd socket stuff, are covered in this dude Pid Eins's &lt;a href="http://0pointer.de/blog/projects/security.html"&gt;blog&lt;/a&gt;, though it's not spectacularly organized.
For implementing systemd for Gunicorn, I referenced mostly &lt;a href="http://docs.gunicorn.org/en/stable/deploy.html"&gt;Gunicorn&lt;/a&gt;, but also &lt;a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-django-with-postgres-nginx-and-gunicorn-on-ubuntu-16-04#create-a-gunicorn-systemd-service-file"&gt;Digital Ocean&lt;/a&gt;.
I might need a systemd service file for &lt;a href="https://www.nginx.com/resources/wiki/start/topics/examples/initscripts/"&gt;nginx&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Execution&lt;/h1&gt;
&lt;p&gt;Gunicorn recommends setting up a socket, which is always listening, which starts up Gunicorn proper via a corresponding "service". Per Eins's blog, this can be a more efficient way to use resources. For instance, if you have a bunch of rarely-accessed websites on the same server, you could activate / deactivate them as needed. I wonder if this is how Heroku manages its hobby-tier apps.&lt;/p&gt;
&lt;p&gt;The gunicorn.socket file is super simple:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[Unit]&lt;/span&gt;
&lt;span class="na"&gt;Description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;gunicorn socket&lt;/span&gt;

&lt;span class="k"&gt;[Socket]&lt;/span&gt;
&lt;span class="na"&gt;ListenStream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/run/gunicorn/socket&lt;/span&gt;

&lt;span class="k"&gt;[Install]&lt;/span&gt;
&lt;span class="na"&gt;WantedBy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;sockets.target&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It basically specifies an address to listen on for a stream socket. systemd appears to create this socket as part of this gunicorn.socket file's existence -- this socket is the only contents of /run/gunicorn when you navigate to it upon starting up the server.&lt;/p&gt;
&lt;p&gt;The gunicorn.service is a bit more complex:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[Unit]&lt;/span&gt;
&lt;span class="na"&gt;Description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;gunicorn daemon&lt;/span&gt;
&lt;span class="na"&gt;Requires&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;gunicorn.socket&lt;/span&gt;
&lt;span class="na"&gt;After&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;network.target&lt;/span&gt;

&lt;span class="k"&gt;[Service]&lt;/span&gt;
&lt;span class="na"&gt;PIDFile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/run/gunicorn/pid&lt;/span&gt;
&lt;span class="na"&gt;User&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;vagrant&lt;/span&gt;
&lt;span class="na"&gt;Group&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;www-data&lt;/span&gt;
&lt;span class="na"&gt;RuntimeDirectory&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;gunicorn&lt;/span&gt;
&lt;span class="na"&gt;WorkingDirectory&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/vagrant/mysite&lt;/span&gt;
&lt;span class="na"&gt;ExecStart&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/home/vagrant/.local/share/virtualenvs/vagrant-gKDsaKU3/bin/gunicorn --pid /run/gunicorn/pid --bind unix:/run/gunicorn/socket mysite.wsgi&lt;/span&gt;
&lt;span class="na"&gt;ExecReload&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/bin/kill -s HUP $MAINPID&lt;/span&gt;
&lt;span class="na"&gt;ExecStop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/bin/kill -s TERM $MAINPID&lt;/span&gt;
&lt;span class="na"&gt;PrivateTmp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;true&lt;/span&gt;

&lt;span class="k"&gt;[Install]&lt;/span&gt;
&lt;span class="na"&gt;WantedBy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;multi-user.target&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is more complex. An annotation (NB: I do not know what all of these mean) goes something like:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Requires=gunicorn.socket&lt;/code&gt; says that gunicorn.service is dependent upon gunicorn.socket.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PIDFile=/run/gunicorn/pid&lt;/code&gt; I'm unsure why Gunicorn's docs have this in here. Look at the systemd docs for more info on it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;User&lt;/code&gt; and &lt;code&gt;Group&lt;/code&gt; per Digital Ocean,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We will give our regular user account ownership of the process since it owns all of the relevant files. We'll give group ownership to the www-data group so that Nginx can communicate easily with Gunicorn.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I am still a bit unclear about what user and group will be needed. I tried setting both &lt;code&gt;User&lt;/code&gt; and &lt;code&gt;Group&lt;/code&gt; to be "vagrant", and tried both as "www-data", and both yield responses when connecting to the index page on localhost.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RuntimeDirectory=gunicorn&lt;/code&gt; The &lt;a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#"&gt;docs&lt;/a&gt; indicate that the directory's lifetime will be bound to the daemon's. Upon running &lt;code&gt;sudo systemctl stop gunicorn.service&lt;/code&gt;, which calls the &lt;code&gt;ExecStop&lt;/code&gt; command, /run/gunicorn is empty, i.e. the socket is gone. It looks like this declaration supersedes the Gunicorn docs's suggestion of having /etc/tmpfiles.d/gunicorn.conf (systemd says tmpfiles.d is for complex cases).
Removing this line yields the following error when trying to connect:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;curl: (56) Recv failure: Connection reset by peer&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And it appears that /run/gunicorn and its parent dirs are owned by root with permissions 755. Conversely, when RuntimeDirectory is specified, /run/gunicorn has User and Group equal to what they were specified as in the gunicorn.service file (vagrant and www-data in this case).
When the service has been halted, then, per RuntimeDirectory's behavior, /run/gunicorn will be deleted. As such, to restart things, you need to restart gunicorn.socket with &lt;code&gt;sudo systemctl restart gunicorn.socket&lt;/code&gt;. This will recreate the /run/gunicorn directory with the socket in it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;WorkingDirectory&lt;/code&gt; sets the directory for any executed commands (e.g. ExecStart).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ExecStart&lt;/code&gt; and company are just commands to execute at the relevant lifecycle events. For my purposes, because I have Gunicorn installed via a virtual environment, I have to provide the path to its actual install. I think I could alternatively use other lifecycle hooks to start and stop the virtual environment, the just call 'gunicorn' in my ExecStart.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PrivateTmp&lt;/code&gt; is described as a security feature (see also Eins's blog)&lt;/p&gt;
&lt;h1&gt;Problems I Encountered&lt;/h1&gt;
&lt;p&gt;I made some edits to my gunicorn.service file, and I was not getting my curl test to work (&lt;code&gt;curl --unix-socket /run/gunicorn/socket http://localhost:8000&lt;/code&gt; &lt;em&gt;mind the port&lt;/em&gt;). Upon restarting the VM, it worked. I believe I needed to run &lt;code&gt;sudo systemctl daemon-restart&lt;/code&gt;.&lt;/p&gt;</content><category term="Network and Web"></category><category term="computer"></category><category term="backend"></category><category term="howto"></category></entry><entry><title>Stuff About Ports and Sockets</title><link href="https://swpease.github.io/stuff-about-ports-and-sockets.html" rel="alternate"></link><published>2018-12-15T00:00:00-08:00</published><updated>2018-12-15T00:00:00-08:00</updated><author><name>Scott Pease</name></author><id>tag:swpease.github.io,2018-12-15:/stuff-about-ports-and-sockets.html</id><summary type="html">&lt;p&gt;Socket = {IP Address : Port}
Connection = {Local Socket + Remote Socket + Protocol}
Protocol = TCP, UDP, etc.
Listen = Look for client requests at a specific well-known port.&lt;/p&gt;
&lt;p&gt;A server socket can serve multiple clients because the service just kinda keeps track of which requests are associated with which clients. Which I suppose makes …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Socket = {IP Address : Port}
Connection = {Local Socket + Remote Socket + Protocol}
Protocol = TCP, UDP, etc.
Listen = Look for client requests at a specific well-known port.&lt;/p&gt;
&lt;p&gt;A server socket can serve multiple clients because the service just kinda keeps track of which requests are associated with which clients. Which I suppose makes complete sense without any thought.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/3329641/how-do-multiple-clients-connect-simultaneously-to-one-port-say-80-on-a-server"&gt;Stack Overflow&lt;/a&gt;: How to multiple clients connect simultaneously to one port?&lt;/p&gt;
&lt;p&gt;When a client connects to a server, the client uses a random, unused, high-number port. That way multiple people with the same IP address (family, coworkers, etc.) can use the same service at the same time.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/1694144/can-two-applications-listen-to-the-same-port"&gt;Stack Overflow&lt;/a&gt;: Can two applications listen to the same port?&lt;/p&gt;
&lt;p&gt;For TCP, no. That's the reason ports exist: to allow multiple applications to share the network without conflicts. Well, technically yes, but not without work.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Port_(computer_networking)"&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For TCP, only one process may bind to a specific IP address and port combination.
Common application failures, sometimes called port conflicts, occur when multiple programs attempt to use the same port number on the same IP address with the same protocol.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.quora.com/What-is-the-difference-between-HTTP-protocol-and-TCP-protocol"&gt;Quora&lt;/a&gt;: The differences between IP, TCP, HTTP, etc.&lt;/p&gt;
&lt;h2&gt;Sockets&lt;/h2&gt;
&lt;p&gt;The answer given by Daniel Miller is quite good.
See also &lt;a href="https://en.wikipedia.org/wiki/Internet_protocol_suite#Internet_layer"&gt;Wikipedia&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Application programmers are typically concerned only with interfaces in the application layer and often also in the transport layer, while the layers below are services provided by the TCP/IP stack in the operating system. Most IP implementations are accessible to programmers through sockets and APIs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are multiple, different definitions of "socket" depending on context. e.g. the &lt;a href="https://en.wikipedia.org/wiki/Berkeley_sockets"&gt;Berkeley socket&lt;/a&gt;, for which a socket is an abstract representation for the local endpoint of a network communication path. Incidentally, Berkeley sockets answers my questions about where IP and transport layer coding is stored / interacted with (a socket API).&lt;/p&gt;
&lt;p&gt;And &lt;a href="https://docs.python.org/3.7/howto/sockets.html"&gt;Python sockets HOWTO&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Useful answer on &lt;a href="https://askubuntu.com/questions/247625/what-is-the-loopback-device-and-how-do-i-use-it"&gt;loopback&lt;/a&gt;
&lt;a href="https://docs.freebsd.org/44doc/psd/20.ipctut/paper.pdf"&gt;Berkeley sockets&lt;/a&gt;&lt;/p&gt;</content><category term="Network and Web"></category><category term="computer"></category><category term="networking"></category><category term="sockets"></category></entry></feed>